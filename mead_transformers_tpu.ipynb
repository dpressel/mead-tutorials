{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mead-transformers-tpu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC2DpvRTxA_I",
        "colab_type": "text"
      },
      "source": [
        "## Training Transformers on TPUs with the API\n",
        "\n",
        "In this example, we will demonstrate code to train Transformers on a TPU using MEAD/Baseline in TensorFlow.  The basic outline of the program is based on the API example [pretrain-tlm-tf](https://github.com/dpressel/mead-baseline/blob/master/api-examples/pretrain-tlm-tf.py).\n",
        "\n",
        "The data for this sample is a preprocessed version of [wikitext-2](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) available from a GCP bucket. It was preprocessed from the original data using the API example [preproc-tlm](https://github.com/dpressel/mead-baseline/blob/master/api-examples/pretrain-tlm-tf.py) with command-line args specified to generate [TFRecords](https://www.tensorflow.org/tutorials/load_data/tfrecord).  To access them, we need to start by authenticating our colab user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0TvXEMqA50x",
        "colab_type": "code",
        "outputId": "6fa19115-31af-4167-ddbe-b331774ef7e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gsutil ls gs://lm-sample\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://lm-sample/wt2/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qh4tPpgyB-1",
        "colab_type": "text"
      },
      "source": [
        "The meta-data we need to process this example is publicly available on dropbox, and was previously processed with [preproc-tlm](https://github.com/dpressel/mead-baseline/blob/master/api-examples/pretrain-tlm-tf.py) (to generate the `YAML` md files), and [fastBPE](https://github.com/glample/fastBPE) was run to generate the vocab and codes files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqCanwBPaT-G",
        "colab_type": "code",
        "outputId": "84b60a07-634a-435e-c1e8-764a42be13dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "\n",
        "!wget https://www.dropbox.com/s/yaqs2dx51kc4sb2/wt2-md.tar.gz?dl=1\n",
        "!tar -xzf wt2-md.tar.gz?dl=1\n",
        "!rm wt2-md.tar.gz?dl=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-01 16:37:32--  https://www.dropbox.com/s/yaqs2dx51kc4sb2/wt2-md.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:6018:1::a27d:301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/yaqs2dx51kc4sb2/wt2-md.tar.gz [following]\n",
            "--2020-06-01 16:37:32--  https://www.dropbox.com/s/dl/yaqs2dx51kc4sb2/wt2-md.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf5f62194856a58ee3953a2b5b2.dl.dropboxusercontent.com/cd/0/get/A40TQap6UgNNNi019U71VtWQf9wOnqlmBSlXi-R1MFqWO6ioTXYo3CiR7OpSXQbc4ya3tPxELhdwjxqaNYLoG0AWcLzAy5bZ3d7BZwfl7i5ZZHzkMcCcDbIzLXQ0ebTwfIs/file?dl=1# [following]\n",
            "--2020-06-01 16:37:32--  https://ucf5f62194856a58ee3953a2b5b2.dl.dropboxusercontent.com/cd/0/get/A40TQap6UgNNNi019U71VtWQf9wOnqlmBSlXi-R1MFqWO6ioTXYo3CiR7OpSXQbc4ya3tPxELhdwjxqaNYLoG0AWcLzAy5bZ3d7BZwfl7i5ZZHzkMcCcDbIzLXQ0ebTwfIs/file?dl=1\n",
            "Resolving ucf5f62194856a58ee3953a2b5b2.dl.dropboxusercontent.com (ucf5f62194856a58ee3953a2b5b2.dl.dropboxusercontent.com)... 162.125.3.6, 2620:100:6018:6::a27d:306\n",
            "Connecting to ucf5f62194856a58ee3953a2b5b2.dl.dropboxusercontent.com (ucf5f62194856a58ee3953a2b5b2.dl.dropboxusercontent.com)|162.125.3.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 438482 (428K) [application/binary]\n",
            "Saving to: ‘wt2-md.tar.gz?dl=1’\n",
            "\n",
            "wt2-md.tar.gz?dl=1  100%[===================>] 428.21K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-06-01 16:37:33 (8.82 MB/s) - ‘wt2-md.tar.gz?dl=1’ saved [438482/438482]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfG2WzmaWgPz",
        "colab_type": "code",
        "outputId": "71a8dbe6-b24d-4107-a05e-d3fce2f3ac48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  mlm-bpe-1871\tsample_data  wt2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuLRafQAy9-p",
        "colab_type": "text"
      },
      "source": [
        "To run our example, we need to install TensorFlow, [fastBPE](https://github.com/glample/fastBPE), and MEAD/Baseline with [TensorFlow addons](https://www.tensorflow.org/addons/overview).  If you get an error at the end of this command, run it a second time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN9XRH6eBnj4",
        "colab_type": "code",
        "outputId": "ef212799-8c2f-4856-da8f-a7dc1676944b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install fastBPE\n",
        "!pip install mead-baseline[tf2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.29.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: fastBPE in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: mead-baseline[tf2] in /usr/local/lib/python3.6/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mead-baseline[tf2]) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from mead-baseline[tf2]) (1.12.0)\n",
            "Requirement already satisfied: mead-layers==2.0.3 in /usr/local/lib/python3.6/dist-packages (from mead-baseline[tf2]) (2.0.3)\n",
            "Requirement already satisfied: tensorflow-addons; extra == \"tf2\" in /usr/local/lib/python3.6/dist-packages (from mead-baseline[tf2]) (0.8.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons; extra == \"tf2\"->mead-baseline[tf2]) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NtsNn6Hzc8G",
        "colab_type": "text"
      },
      "source": [
        "To run our example, we will need to import the `BPEVectorizer1D` which is reponsible for vectorizing text to BPE form, a few utilities from 8-mile including the entire optimizer base and TF packages, as well as the language models we will be using from the `baseline.tf.lm` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg4tATmaB4tx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import os\n",
        "from argparse import ArgumentParser\n",
        "import math\n",
        "from typing import Tuple\n",
        "import baseline\n",
        "from eight_mile.utils import str2bool, write_json\n",
        "import baseline.tf.embeddings\n",
        "import baseline.embeddings\n",
        "from baseline.vectorizers import BPEVectorizer1D\n",
        "from eight_mile.utils import Average, get_num_gpus_multiworker, read_yaml\n",
        "from eight_mile.optz import *\n",
        "from eight_mile.tf.optz import *\n",
        "from baseline.tf.lm import SET_TRAIN_FLAG, TransformerLanguageModel, TransformerMaskedLanguageModel\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import json\n",
        "\n",
        "logger = logging.getLogger(\"mead-transformers-tpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw-kvOVd0PNI",
        "colab_type": "text"
      },
      "source": [
        "Here we will define the loss as a function object -- a class with an overloaded `__call__()` function.  We will instantiate using the normal constructor (which takes the BPE `vocab_size` and the context window size), but when its called during optimization, the instance name is called with parens (containing the `model`, `features` and `labels`) as though it was a normal function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZLqR4aGCWY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Loss:\n",
        "    def __init__(self, vocab_size, nctx):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.nctx = nctx\n",
        "\n",
        "    def __call__(self, model, features, labels):\n",
        "        logits, _ = model(features, None)\n",
        "        loss_mask = tf.cast(labels != 0, tf.float32)\n",
        "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
        "        losses = losses * loss_mask\n",
        "        losses = tf.reduce_sum(losses)\n",
        "        non_zero = tf.reduce_sum(loss_mask)\n",
        "        losses /= non_zero\n",
        "        return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eaaBALe0wXY",
        "colab_type": "text"
      },
      "source": [
        "Our preprocessed data files are written in shards of TFRecords (of size ~100MB each) with the feature `x` representing the integer values of the input BPE tokens and `y` representing the target integer values to recover during masked language modeling).\n",
        "\n",
        "Here is an example of what these records would look like as a JSON object:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"x\": [4, 10, 99, 1926, 21, 128, 11, 106, 5, 13288, 33, 7, 5409, 565, 399, 6, 26, 10, 20, 1191, 153, 283, 13, 39, 399, 57, 6, 10, 49, 1616, 5, 63, 1822, 11, 106, 58, 5, 10, 117, 183, 195, 283, 18, 7, 295, 11, 59, 6, 5, 5, 99, 1238, 13, 484, 5, 11723, 852, 18, 2652, 8, 330, 292, 5, 50, 5, 88, 11, 6, 168, 13, 75, 87, 5, 830, 7820, 3012, 18, 834, 9559, 13, 135, 1175, 19, 32, 49, 134, 13, 11, 53, 15, 7, 5939, 1069, 30, 2, 86, 1238, 5, 382, 5129, 8, 4882, 4211, 3383, 42, 5, 19213, 6, 7, 3594, 5, 33, 5129, 5, 221, 17, 140, 8, 38, 5, 49, 127, 19, 7, 11963, 39, 2549, 131, 6, 2, 100, 1238, 13, 39, 28, 18, 2562, 18, 4428, 14327, 7, 44, 12, 1610, 5, 979, 2021, 194, 221, 51, 6, 26, 10, 5, 8432, 7, 3594, 44, 379, 3010, 5, 495, 33, 19, 71, 6, 5, 47, 1713, 174, 16, 644, 15, 231, 7, 3442, 3378, 6, 5, 5, 37, 10, 52, 7, 632, 5, 13, 8, 13, 356, 52, 12, 5, 5, 5, 194, 7, 700, 15, 7, 5, 8, 9311, 28, 1090, 5, 84, 657, 11858, 14043, 8, 9, 294, 27, 28, 37, 63, 6772, 42, 2697, 37, 5, 2488, 19, 31, 347, 21, 7, 971, 41, 5, 2299, 5, 5, 20151, 1645, 37, 11, 66, 294, 5, 18, 7, 5, 15, 7, 5, 17, 233, 19, 92, 56, 13, 5, 86, 5],\n",
        "  \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 10, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 17, 0, 11, 0, 0, 0, 0, 0, 0, 0, 18408, 0, 0, 0, 0, 834, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 15380, 0, 0, 0, 0, 20, 0, 0, 194, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 7, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 83, 127, 0, 0, 0, 0, 0, 6605, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 118, 0, 0, 0, 7, 0, 15, 0, 0, 0, 0, 0, 0, 878, 19, 416, 0, 0, 0, 0, 0, 342, 0, 46, 0, 0, 6002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3442, 0, 0, 0, 0, 0, 0, 0, 0, 2839, 0, 6, 83, 72, 0, 37, 0, 0, 0, 28, 0, 0, 668, 0, 0, 3378, 0, 0, 0, 0, 0, 0, 30, 0, 644]\n",
        "}\n",
        "```\n",
        "\n",
        "To read the TFRecords, we need a descriptor of these features, and we need to use the [tf.io.parse_single_example()](https://www.tensorflow.org/api_docs/python/tf/io/parse_single_example) function, which we will apply to each record.\n",
        "\n",
        "These files are stored on a GCP bucket, so when we call `get_dataset()` below, we will `glob` the bucket for `*.tfrecord` files then map each record retreival to a pipeline that includes parsing the example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sha4CdqHCb9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "feature_description = {\n",
        "    'x': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True, default_value=0),\n",
        "    'y': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True, default_value=0),\n",
        "}\n",
        "\n",
        "\n",
        "def _parse_tf_record(example_proto):\n",
        "    record = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    return record['x'], record['y']\n",
        "\n",
        "\n",
        "def get_dataset(directory):\n",
        "    pattern = os.path.join(directory, f'*.tfrecord')\n",
        "    files = tf.io.gfile.glob(pattern)\n",
        "    print(files)\n",
        "    ds = tf.data.TFRecordDataset(files).map(_parse_tf_record)\n",
        "    return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXUJzFdL2hPY",
        "colab_type": "text"
      },
      "source": [
        "Our example will use the `tf.distribute` library, which allows us to provide a [tf.distribute.Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) for handling distribution details. We just need to create the right sub-class, in this case a [TPUStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy), which we will need to initialize with a [TPUClusterResolver](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver) that takes in an address for the TPU.  Because this example is in colab, we are going to end up calling this function with an empty address and it will use the environment variable `COLAB_TPU_ADDR` to resolve our TPU address."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmK8E16KCmnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_distribute_strategy(strategy_name, endpoint=None):\n",
        "    if strategy_name == 'tpu':\n",
        "        if endpoint is None:\n",
        "            endpoint = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=endpoint)\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        # This is the TPU initialization code that has to be at the beginning.\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "    elif strategy_name == 'mirror':\n",
        "        num_gpus = get_num_gpus_multiworker()\n",
        "        devices = ['/device:GPU:{}'.format(i) for i in range(num_gpus)]\n",
        "        strategy = tf.distribute.MirroredStrategy(devices)\n",
        "    else:\n",
        "        raise Exception(f\"Unsupported strategy {strategy_name}\")\n",
        "    return strategy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKJrbuSx32bi",
        "colab_type": "text"
      },
      "source": [
        "Now that we have set up a lot of the boilerplate we can go ahead and create our\n",
        "1. `tf.distribute.Strategy` using the function above\n",
        "2. `baseline.Vectorizer` which we will use to initialize the vocabulary of our model\n",
        "3. `baseline.embeddings` whch we will use to initalize a lookup table that projects from our vocabulary size to a hidden unit size (`MLM_MODEL_SZ` below)\n",
        "4. `baseline.tf.lm.TransformerMaskedLanguageModel` which is the model we will be training.  It is built internally on `8 mile` by using an [eight_mile.tf.layers.TransformerEncoderStack](https://github.com/dpressel/mead-baseline/blob/master/layers/eight_mile/tf/layers.py), which composes a stack of `Transformer` encoders with interleaved multi-head attention and FFN sub-stacks.\n",
        "\n",
        "The `TransformerMaskedLanguageModel` is a type of `class AbstractGeneratorModel(LanguageModelBase)`, which defines the abstract phases to create a `LanguageModel`:\n",
        "\n",
        "```python\n",
        "    def create_layers(self, embeddings, **kwargs):\n",
        "        self.embeddings = self.init_embed(embeddings, **kwargs)\n",
        "        self.embeddings_proj = self.init_embeddings_proj(**kwargs)\n",
        "        self.generator = self.init_generate(**kwargs)\n",
        "        self.output_layer = self.init_output(embeddings, **kwargs)\n",
        "```\n",
        "\n",
        "This gets sub-classed by the `TransformerLanguageModel` to provide a `Transformer`-based `generator` object:\n",
        "\n",
        "```python\n",
        "    def init_generate(self, **kwargs):\n",
        "        pdrop = float(kwargs.get('dropout', 0.1))\n",
        "        layers = kwargs.get('layers', kwargs.get('num_layers', 1))\n",
        "        d_model = int(kwargs.get('d_model', kwargs.get('hsz')))\n",
        "        num_heads = kwargs.get('num_heads', 4)\n",
        "        d_ff = int(kwargs.get('d_ff', 4 * d_model))\n",
        "        rpr_k = kwargs.get('rpr_k')\n",
        "        d_k = kwargs.get('d_k')\n",
        "        scale = bool(kwargs.get('scale', True))\n",
        "        activation = kwargs.get('activation', 'gelu')\n",
        "        layer_norm_eps = kwargs.get('layer_norm_eps', 1e-12)\n",
        "        layer_norms_after = kwargs.get('layer_norms_after', False)\n",
        "        return TransformerEncoderStack(num_heads, d_model=d_model, pdrop=pdrop, scale=scale,\n",
        "                                       layers=layers, d_ff=d_ff, rpr_k=rpr_k, d_k=d_k,\n",
        "                                       activation=activation, layer_norm_eps=layer_norm_eps,\n",
        "                                       layer_norms_after=layer_norms_after)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aioXP-exCsFL",
        "colab_type": "code",
        "outputId": "64682972-eadb-4132-c57c-7230f667f39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "\n",
        "\n",
        "MLM_MODEL_SZ = 512\n",
        "MLM_FFN_SZ = 4 * MLM_MODEL_SZ\n",
        "MLM_CONTEXT_LENGTH = 256\n",
        "MLM_REL_POS_REPR = 8\n",
        "MLM_DROPOUT = 0.1\n",
        "MLM_NUM_HEADS = 8\n",
        "MLM_NUM_LAYERS = 8\n",
        "SET_TRAIN_FLAG(True)\n",
        "\n",
        "SUBWORD_MODEL_FILE = './wt2/wiki.train.bpe.50k.codes'\n",
        "SUBWORD_VOCAB_FILE = './wt2/wiki.train.bpe.50k.vocab'\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "strategy = create_distribute_strategy(\"tpu\")\n",
        "num_replicas = strategy.num_replicas_in_sync\n",
        "logger.info(f\"Using {num_replicas} replicas in this job.\")\n",
        "vectorizer = BPEVectorizer1D(model_file=SUBWORD_MODEL_FILE,\n",
        "                             vocab_file=SUBWORD_VOCAB_FILE,\n",
        "                             mxlen=MLM_MODEL_SZ)\n",
        "vocab = {'x': vectorizer.vocab}\n",
        "preproc_data = baseline.embeddings.load_embeddings('x',\n",
        "                                                   dsz=MLM_MODEL_SZ,\n",
        "                                                   known_vocab=vocab['x'],\n",
        "                                                   preserve_vocab_indices=True,\n",
        "                                                   embed_type=\"default\")\n",
        "vocabs = preproc_data['vocab']\n",
        "vocab_size = max(vocabs.values())\n",
        "embeddings = {'x': preproc_data['embeddings']}\n",
        "\n",
        "model = TransformerMaskedLanguageModel.create(embeddings,\n",
        "                                              hsz=MLM_MODEL_SZ,\n",
        "                                              d_ff=MLM_FFN_SZ,\n",
        "                                              tie_weights=True,\n",
        "                                              dropout=MLM_DROPOUT,\n",
        "                                              num_heads=MLM_NUM_HEADS,\n",
        "                                              layers=MLM_NUM_LAYERS,\n",
        "                                              rpr_k=MLM_REL_POS_REPR,\n",
        "                                              src_keys=['x'], tgt_key='x')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.101.52.138:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.101.52.138:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU')]\n",
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "INFO:mead-transformers-tpu:Using 8 replicas in this job.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4bSuLW26L0D",
        "colab_type": "text"
      },
      "source": [
        "We have defined our model and our loss function, as well as the initial step of our data pipeline using `TFRecordDataset`.  Now we will provide functions that will distribute our reader over each replica using the `strategy.experimental_distribute_datasets_from_function` API call, and batches the dataset into our per-replica dataset size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hxAutT7EVCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 8 * 20\n",
        "TRAIN_BUCKET = \"gs://lm-sample/wt2/train\"\n",
        "VALID_BUCKET = \"gs://lm-sample/wt2/valid\"\n",
        "MD_TRAIN_FILE = \"./wt2/train/md.yml\"\n",
        "MD_TEST_FILE = \"./wt2/valid/md.yml\"\n",
        "def get_num_samples(f):\n",
        "    yml = read_yaml(f)\n",
        "    return yml['num_samples']\n",
        "\n",
        "def dataset_train_fn(input_context):\n",
        "    batch_size = input_context.get_per_replica_batch_size(BATCH_SIZE)\n",
        "    ds = get_dataset(TRAIN_BUCKET).batch(batch_size)\n",
        "    return ds.shard(\n",
        "        input_context.num_input_pipelines, input_context.input_pipeline_id\n",
        "    )\n",
        "train_loader = strategy.experimental_distribute_datasets_from_function(dataset_train_fn)\n",
        "\n",
        "def dataset_test_fn(input_context):\n",
        "    batch_size = input_context.get_per_replica_batch_size(BATCH_SIZE)\n",
        "    ds = get_dataset(VALID_BUCKET).batch(batch_size)\n",
        "    return ds.shard(\n",
        "        input_context.num_input_pipelines, input_context.input_pipeline_id\n",
        "    )\n",
        "valid_loader = strategy.experimental_distribute_datasets_from_function(dataset_test_fn)\n",
        "\n",
        "num_train_samples = get_num_samples(MD_TRAIN_FILE)\n",
        "num_valid_samples = get_num_samples(MD_TEST_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_X1YPjV69pu",
        "colab_type": "text"
      },
      "source": [
        "Now we can create our loss function, and set some variables so we can get periodic updates while we train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WumPrXLvGDlp",
        "colab_type": "code",
        "outputId": "aa5ddced-c3bc-414f-c81f-72e5e656c6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss_function = Loss(vocab_size, MLM_CONTEXT_LENGTH)\n",
        "steps_per_epoch = num_train_samples // BATCH_SIZE\n",
        "steps_per_valid_epoch = num_valid_samples // BATCH_SIZE\n",
        "update_on = steps_per_epoch // 2\n",
        "report_on = update_on // 4\n",
        "logger.info(f\"Steps per epoch: {steps_per_epoch}. Update every {update_on} steps.\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:mead-transformers-tpu:Steps per epoch: 56. Update every 28 steps.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFGrzt3t7J05",
        "colab_type": "text"
      },
      "source": [
        "The `8 mile` API provides an `EagerOptimizer` which takes in a loss function (or function object in our case) and applies it for every step of training to provide a per-replica loss.  We are going to create a learning regimen that starts with a linear warmup to the target learning rate over `WARMUP_STEPS` composed with learning rate scheduler that provides a cosine decay.\n",
        "\n",
        "We will specify that our model is to be trained with [adamw](https://www.fast.ai/2018/07/02/adam-weight-decay/) (we use the `tensorflow_addons` implementation underneath) with weight decay given by `WD` below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMhh55X9IMCx",
        "colab_type": "code",
        "outputId": "67009c9c-8f53-4cc2-c40f-9b97633150b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "LR = 1.0e-4 * 8\n",
        "EPOCHS = 10\n",
        "GRAD_CLIP = 1.0\n",
        "OPTIM = \"adamw\"\n",
        "WD = 1.0e-5\n",
        "WARMUP_STEPS = 10000\n",
        "lr_decay = CosineDecaySchedulerTensorFlow(steps_per_epoch)\n",
        "linear_warmup = WarmupLinearSchedulerTensorFlow(WARMUP_STEPS, lr=LR)\n",
        "lr_sched = CompositeLRSchedulerTensorFlow(linear_warmup, lr_decay, lr=LR)\n",
        "\n",
        "optimizer = EagerOptimizer(loss_function, global_step=1, lr=LR, optim=OPTIM,\n",
        "                           learning_rate_decay_fn=lr_sched,\n",
        "                           weight_decay=WD, clip=GRAD_CLIP)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:mead.layers:adamw(eta=0.000800 beta1=0.900000, beta2=0.999000, eps=0.000000, wd=0.000010)\n",
            "INFO:mead.layers:clip gradients at 1.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Krt3L58JII",
        "colab_type": "text"
      },
      "source": [
        "We are going to define 2 autograph compiled functions, one for distributed training and one for distributed testing.  The implementations proxy to an underlying per-replica function which optimize and provide back the per-replica losses, which are accumulated in the distributed function.  These functions use the `strategy.experimental_run_v2` function to call the per-replica versions, and `strategy.reduce` to sum the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLrkWby-LcqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def _replicated_train_step(inputs):\n",
        "    x, y = inputs\n",
        "    per_replica_loss = optimizer.update(model, {'x': x}, y, num_replicas)\n",
        "    return per_replica_loss\n",
        "\n",
        "@tf.function\n",
        "def _distributed_train_step(inputs: Tuple[tf.Tensor, tf.Tensor]):\n",
        "    per_replica_loss = strategy.experimental_run_v2(_replicated_train_step, args=(inputs,))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
        "\n",
        "valid_loss_function = Loss(vocab_size, MLM_CONTEXT_LENGTH)\n",
        "def _replicated_test_step(inputs):\n",
        "    x, y = inputs\n",
        "    per_replica_loss = valid_loss_function(model, {'x': x}, y) / num_replicas\n",
        "    return per_replica_loss\n",
        "\n",
        "@tf.function\n",
        "def _distributed_test_step(inputs: Tuple[tf.Tensor, tf.Tensor]):\n",
        "    per_replica_loss = strategy.experimental_run_v2(_replicated_test_step, args=(inputs,))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFZ56p-B8oIx",
        "colab_type": "text"
      },
      "source": [
        "We have finally set up all the boilerplate and we can run a normal training loop.  The only difference for a distributed program within the training loop is that we call the operations within a `strategy.scope()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMTTpSP_MCHL",
        "colab_type": "code",
        "outputId": "c8772437-74f8-4670-ff16-9a05a6de51b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "steps = 1\n",
        "start_epoch = 0\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "    SET_TRAIN_FLAG(True)\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        avg_loss = Average('average_train_loss')\n",
        "        metrics = {}\n",
        "        start = time.time()\n",
        "        train_iter = iter(train_loader)\n",
        "        for i in range(steps_per_epoch):\n",
        "            steps += 1\n",
        "            loss = _distributed_train_step(next(train_iter))\n",
        "            avg_loss.update(loss.numpy().item())\n",
        "            if (i + 1) % report_on == 0:\n",
        "                logging.info(avg_loss)\n",
        "            if (i + 1) % update_on == 0:\n",
        "                elapsed = (time.time() - start)/60\n",
        "                ##print(avg_loss.avg, math.exp(avg_loss.avg))\n",
        "                logging.info('elapsed time this epoch %d min', elapsed)\n",
        "                logging.info('elapsed step time %f steps/min', i/elapsed)\n",
        "\n",
        "        # How much time elapsed in minutes\n",
        "        elapsed = (time.time() - start)/60\n",
        "        train_token_loss = avg_loss.avg\n",
        "        # This is the average training token-level loss across all machines\n",
        "        # This is the token-level training perplexity\n",
        "        train_token_ppl = math.exp(train_token_loss)\n",
        "        metrics['train_elapsed_min'] = elapsed\n",
        "        metrics['average_train_loss'] = train_token_loss\n",
        "        metrics['average_train_token_ppl'] = train_token_ppl\n",
        "        avg_valid_loss = Average('average_valid_loss')\n",
        "        start = time.time()\n",
        "        SET_TRAIN_FLAG(False)\n",
        "        valid_iter = iter(valid_loader)\n",
        "        for i in range(steps_per_valid_epoch):\n",
        "            valid_loss = _distributed_test_step(next(valid_iter))\n",
        "            avg_valid_loss.update(valid_loss.numpy().item())\n",
        "\n",
        "        valid_token_loss = avg_valid_loss.avg\n",
        "        valid_token_ppl = math.exp(valid_token_loss)\n",
        "\n",
        "        elapsed = (time.time() - start)/60\n",
        "        metrics['valid_elapsed_min'] = elapsed\n",
        "        metrics['average_valid_loss'] = valid_token_loss\n",
        "        metrics['average_valid_token_ppl'] = valid_token_ppl\n",
        "        print(json.dumps(metrics, indent=4, sort_keys=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 7.089441 (7.075352)\n",
            "INFO:root:average_train_loss 6.983522 (7.072307)\n",
            "INFO:root:average_train_loss 7.122105 (7.080230)\n",
            "INFO:root:average_train_loss 6.993566 (7.073015)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 31.587442 steps/min\n",
            "INFO:root:average_train_loss 7.015446 (7.071518)\n",
            "INFO:root:average_train_loss 7.071340 (7.078360)\n",
            "INFO:root:average_train_loss 7.175150 (7.081775)\n",
            "INFO:root:average_train_loss 7.149497 (7.087573)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 56.690000 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.087573375020709,\n",
            "    \"average_train_token_ppl\": 1196.9996043469598,\n",
            "    \"average_valid_loss\": 6.938916206359863,\n",
            "    \"average_valid_token_ppl\": 1031.6515115010525,\n",
            "    \"train_elapsed_min\": 0.9702444513638814,\n",
            "    \"valid_elapsed_min\": 0.07268238464991252\n",
            "}\n",
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 6.971631 (6.975532)\n",
            "INFO:root:average_train_loss 6.896413 (6.974734)\n",
            "INFO:root:average_train_loss 7.057155 (6.988991)\n",
            "INFO:root:average_train_loss 6.938303 (6.988653)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 217.506247 steps/min\n",
            "INFO:root:average_train_loss 6.951414 (6.993201)\n",
            "INFO:root:average_train_loss 7.001421 (6.999779)\n",
            "INFO:root:average_train_loss 7.125483 (7.006451)\n",
            "INFO:root:average_train_loss 7.086274 (7.014928)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 228.429860 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.014928187642779,\n",
            "    \"average_train_token_ppl\": 1113.1267070885322,\n",
            "    \"average_valid_loss\": 6.9435882568359375,\n",
            "    \"average_valid_token_ppl\": 1036.4827164652493,\n",
            "    \"train_elapsed_min\": 0.2408829132715861,\n",
            "    \"valid_elapsed_min\": 0.01825094223022461\n",
            "}\n",
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 6.949949 (6.972112)\n",
            "INFO:root:average_train_loss 6.888988 (6.968611)\n",
            "INFO:root:average_train_loss 7.066749 (6.981960)\n",
            "INFO:root:average_train_loss 6.911725 (6.984062)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 218.363423 steps/min\n",
            "INFO:root:average_train_loss 6.959283 (6.988287)\n",
            "INFO:root:average_train_loss 6.978216 (6.993257)\n",
            "INFO:root:average_train_loss 7.109393 (6.997729)\n",
            "INFO:root:average_train_loss 7.081411 (7.006621)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 229.776564 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.006621216024671,\n",
            "    \"average_train_token_ppl\": 1103.9182950867048,\n",
            "    \"average_valid_loss\": 6.945255374908447,\n",
            "    \"average_valid_token_ppl\": 1038.2120966736757,\n",
            "    \"train_elapsed_min\": 0.23940510749816896,\n",
            "    \"valid_elapsed_min\": 0.018021090825398763\n",
            "}\n",
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 6.937329 (6.982219)\n",
            "INFO:root:average_train_loss 6.914935 (6.972902)\n",
            "INFO:root:average_train_loss 7.038431 (6.983937)\n",
            "INFO:root:average_train_loss 6.900294 (6.978404)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 219.663637 steps/min\n",
            "INFO:root:average_train_loss 6.969196 (6.983082)\n",
            "INFO:root:average_train_loss 7.082540 (7.005794)\n",
            "INFO:root:average_train_loss 7.117290 (7.015462)\n",
            "INFO:root:average_train_loss 7.050060 (7.019021)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 230.590045 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.019020863941738,\n",
            "    \"average_train_token_ppl\": 1117.6917095470744,\n",
            "    \"average_valid_loss\": 6.979404735565185,\n",
            "    \"average_valid_token_ppl\": 1074.2786967269392,\n",
            "    \"train_elapsed_min\": 0.23855806191762288,\n",
            "    \"valid_elapsed_min\": 0.01808029810587565\n",
            "}\n",
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 6.999094 (7.022191)\n",
            "INFO:root:average_train_loss 6.926431 (7.010803)\n",
            "INFO:root:average_train_loss 7.144818 (7.032847)\n",
            "INFO:root:average_train_loss 6.982699 (7.044690)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 217.842766 steps/min\n",
            "INFO:root:average_train_loss 6.976415 (7.045303)\n",
            "INFO:root:average_train_loss 7.050766 (7.052213)\n",
            "INFO:root:average_train_loss 7.113155 (7.053249)\n",
            "INFO:root:average_train_loss 7.044514 (7.052607)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 228.551451 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.052606642246246,\n",
            "    \"average_train_token_ppl\": 1155.867753089225,\n",
            "    \"average_valid_loss\": 6.956824016571045,\n",
            "    \"average_valid_token_ppl\": 1050.2925428358872,\n",
            "    \"train_elapsed_min\": 0.24068913062413533,\n",
            "    \"valid_elapsed_min\": 0.01954260269800822\n",
            "}\n",
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 7.016996 (7.027875)\n",
            "INFO:root:average_train_loss 6.958185 (7.035856)\n",
            "INFO:root:average_train_loss 7.132082 (7.055464)\n",
            "INFO:root:average_train_loss 6.961725 (7.056203)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 216.214068 steps/min\n",
            "INFO:root:average_train_loss 7.000763 (7.054603)\n",
            "INFO:root:average_train_loss 7.048105 (7.062166)\n",
            "INFO:root:average_train_loss 7.126616 (7.063251)\n",
            "INFO:root:average_train_loss 7.084837 (7.064421)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 229.089940 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.064421279089792,\n",
            "    \"average_train_token_ppl\": 1169.6049007445342,\n",
            "    \"average_valid_loss\": 6.942471885681153,\n",
            "    \"average_valid_token_ppl\": 1035.3262626940764,\n",
            "    \"train_elapsed_min\": 0.24015008608500163,\n",
            "    \"valid_elapsed_min\": 0.01828513542811076\n",
            "}\n",
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 6.998410 (7.023473)\n",
            "INFO:root:average_train_loss 6.926228 (7.013489)\n",
            "INFO:root:average_train_loss 7.112935 (7.030364)\n",
            "INFO:root:average_train_loss 6.970592 (7.036053)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 215.566643 steps/min\n",
            "INFO:root:average_train_loss 6.992733 (7.038232)\n",
            "INFO:root:average_train_loss 7.018197 (7.044628)\n",
            "INFO:root:average_train_loss 7.112901 (7.077833)\n",
            "INFO:root:average_train_loss 7.080790 (7.075710)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 227.145991 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.0757095984050205,\n",
            "    \"average_train_token_ppl\": 1182.8825746549512,\n",
            "    \"average_valid_loss\": 6.940936183929443,\n",
            "    \"average_valid_token_ppl\": 1033.7375305604623,\n",
            "    \"train_elapsed_min\": 0.24223616123199462,\n",
            "    \"valid_elapsed_min\": 0.017551032702128093\n",
            "}\n",
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 7.021813 (7.036692)\n",
            "INFO:root:average_train_loss 6.927490 (7.029205)\n",
            "INFO:root:average_train_loss 7.088665 (7.038347)\n",
            "INFO:root:average_train_loss 6.951767 (7.034799)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 218.921018 steps/min\n",
            "INFO:root:average_train_loss 6.998722 (7.034758)\n",
            "INFO:root:average_train_loss 7.028143 (7.042662)\n",
            "INFO:root:average_train_loss 7.117557 (7.044002)\n",
            "INFO:root:average_train_loss 7.061752 (7.044619)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 231.270327 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.044618563992636,\n",
            "    \"average_train_token_ppl\": 1146.6713706393016,\n",
            "    \"average_valid_loss\": 6.9357068061828615,\n",
            "    \"average_valid_token_ppl\": 1028.3458364119842,\n",
            "    \"train_elapsed_min\": 0.23789680004119873,\n",
            "    \"valid_elapsed_min\": 0.01736583709716797\n",
            "}\n",
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 7.008987 (7.018630)\n",
            "INFO:root:average_train_loss 6.920163 (7.015868)\n",
            "INFO:root:average_train_loss 7.086972 (7.028645)\n",
            "INFO:root:average_train_loss 6.942828 (7.026551)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 220.335643 steps/min\n",
            "INFO:root:average_train_loss 6.978358 (7.025822)\n",
            "INFO:root:average_train_loss 7.010882 (7.032273)\n",
            "INFO:root:average_train_loss 7.114468 (7.033459)\n",
            "INFO:root:average_train_loss 7.066011 (7.035308)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 230.143539 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.035307569163186,\n",
            "    \"average_train_token_ppl\": 1136.0442706486592,\n",
            "    \"average_valid_loss\": 6.935028457641602,\n",
            "    \"average_valid_token_ppl\": 1027.6484960605894,\n",
            "    \"train_elapsed_min\": 0.23902015288670858,\n",
            "    \"valid_elapsed_min\": 0.01749055782953898\n",
            "}\n",
            "['gs://lm-sample/wt2/train/train-1.tfrecord']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 7.004523 (7.016095)\n",
            "INFO:root:average_train_loss 6.914292 (7.010748)\n",
            "INFO:root:average_train_loss 7.080095 (7.021781)\n",
            "INFO:root:average_train_loss 6.939723 (7.020098)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 217.255786 steps/min\n",
            "INFO:root:average_train_loss 6.974094 (7.019442)\n",
            "INFO:root:average_train_loss 7.014464 (7.026440)\n",
            "INFO:root:average_train_loss 7.108461 (7.027479)\n",
            "INFO:root:average_train_loss 7.059165 (7.029342)\n",
            "INFO:root:elapsed time this epoch 0 min\n",
            "INFO:root:elapsed step time 227.802929 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt2/valid/valid-1.tfrecord']\n",
            "{\n",
            "    \"average_train_loss\": 7.029342080865588,\n",
            "    \"average_train_token_ppl\": 1129.287385942992,\n",
            "    \"average_valid_loss\": 6.934982681274414,\n",
            "    \"average_valid_token_ppl\": 1027.6014551223839,\n",
            "    \"train_elapsed_min\": 0.24149733384450275,\n",
            "    \"valid_elapsed_min\": 0.01960286299387614\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0cE-Ije8-xY",
        "colab_type": "text"
      },
      "source": [
        "### Wrap-Up and Next Steps\n",
        "\n",
        "In this example, we used the MEAD/Baseline API code to train a Transformer Language Model on TPUs.  To keep things very simple, we did not write out any checkpoints or any training logs.  To see how you would do this with the `CheckpointManager`, see the [full API example](https://github.com/dpressel/mead-baseline/blob/master/api-examples/pretrain-tlm-tf.py).\n",
        "\n",
        "Also, as this is just a sample, we used a very small dataset [wikitext-2](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/).  To make a compelling MLM, you would want to use a much larger dataset and probably spend a bit more time with the hyper-parameters."
      ]
    }
  ]
}