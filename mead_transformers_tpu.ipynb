{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mead-transformers-tpu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC2DpvRTxA_I"
      },
      "source": [
        "## Training Transformers on TPUs with the API\n",
        "\n",
        "In this example, we will demonstrate code to train Transformers on a TPU using MEAD/Baseline in TensorFlow.  The basic outline of the program is based on the API example [pretrain-tlm-tf](https://github.com/dpressel/mead-baseline/blob/master/api-examples/pretrain_tlm_tf.py):\n",
        "\n",
        "```\n",
        "python preproc_tlm.py --text /data/wt103/wiki.train --codes /data/wt103/codes.30k --vocab /data/wt103/vocab.30k --nctx 256 --prefix \"[CLS]\" --fmt tfrecord --fields x y --output /data/wt103/train/train\n",
        "python preproc_tlm.py --text /data/wt103/wiki.valid --codes /data/wt103/codes.30k --vocab /data/wt103/vocab.30k --nctx 256 --prefix \"[CLS]\" --fmt tfrecord --fields x y --output /data/wt103/valid/valid\n",
        "```\n",
        "\n",
        "The data for this sample is a preprocessed version of [wikitext-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) available from a GCP bucket. It was preprocessed from the original data using the API example [preproc-tlm](https://github.com/dpressel/mead-baseline/blob/master/api-examples/pretrain-tlm-tf.py) with command-line args specified to generate [TFRecords](https://www.tensorflow.org/tutorials/load_data/tfrecord).  To access them, we need to start by authenticating our colab user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0TvXEMqA50x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6c35d1-6cfc-4e2b-8967-db06dd6a9ba4"
      },
      "source": [
        "from google.colab import auth\n",
        "import os\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gsutil ls gs://lm-sample\n",
        "os.environ['COLAB_SKIP_TPU_AUTH'] = '1'"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://lm-sample/wt103/\n",
            "gs://lm-sample/wt2/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qh4tPpgyB-1"
      },
      "source": [
        "The meta-data we need to process this example is publicly available on dropbox, and was previously processed with [preproc-tlm](https://github.com/dpressel/mead-baseline/blob/master/api-examples/pretrain-tlm-tf.py) (to generate the `YAML` md files), and [fastBPE](https://github.com/glample/fastBPE) was run to generate the vocab and codes files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqCanwBPaT-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277f9e59-81ad-4c4f-97d8-9179a37d97d9"
      },
      "source": [
        "\n",
        "!wget https://www.dropbox.com/s/5zgh532d72yhhqm/wt103-md.tar.gz?dl=1\n",
        "!tar -xzf wt103-md.tar.gz?dl=1\n",
        "!rm wt103-md.tar.gz?dl=1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-04 17:59:09--  https://www.dropbox.com/s/5zgh532d72yhhqm/wt103-md.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/5zgh532d72yhhqm/wt103-md.tar.gz [following]\n",
            "--2021-01-04 17:59:09--  https://www.dropbox.com/s/dl/5zgh532d72yhhqm/wt103-md.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc692c488efd70d3f0c19363e86a.dl.dropboxusercontent.com/cd/0/get/BGYBkFwlYIC8WHRE9HePmjWc_XhU-S_WhfE7lrKHRmn_BHEXfWF5tfda6qeiysM9kI9PAnYtNSDWWehr8PxG2iw0Yxt_VwB3JDAsL5NL7Luwqg2Y1DgPil5bmoqAlxSM5i0/file?dl=1# [following]\n",
            "--2021-01-04 17:59:09--  https://uc692c488efd70d3f0c19363e86a.dl.dropboxusercontent.com/cd/0/get/BGYBkFwlYIC8WHRE9HePmjWc_XhU-S_WhfE7lrKHRmn_BHEXfWF5tfda6qeiysM9kI9PAnYtNSDWWehr8PxG2iw0Yxt_VwB3JDAsL5NL7Luwqg2Y1DgPil5bmoqAlxSM5i0/file?dl=1\n",
            "Resolving uc692c488efd70d3f0c19363e86a.dl.dropboxusercontent.com (uc692c488efd70d3f0c19363e86a.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc692c488efd70d3f0c19363e86a.dl.dropboxusercontent.com (uc692c488efd70d3f0c19363e86a.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 334529 (327K) [application/binary]\n",
            "Saving to: ‘wt103-md.tar.gz?dl=1’\n",
            "\n",
            "wt103-md.tar.gz?dl= 100%[===================>] 326.69K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-01-04 17:59:10 (7.22 MB/s) - ‘wt103-md.tar.gz?dl=1’ saved [334529/334529]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfG2WzmaWgPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bd75e7-5180-4a4e-df08-9ff82b141953"
      },
      "source": [
        "!ls\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  sample_data  wt103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuLRafQAy9-p"
      },
      "source": [
        "To run our example, we need to install TensorFlow, [fastBPE](https://github.com/glample/fastBPE), and MEAD/Baseline with [TensorFlow addons](https://www.tensorflow.org/addons/overview).  If you get an error at the end of this command, run it a second time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN9XRH6eBnj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514a9cfe-cd96-4733-8c39-a79fb67b81e6"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install fastBPE\n",
        "!pip install mead-baseline[tf2]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (51.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: fastBPE in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: mead-baseline[tf2] in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mead-baseline[tf2]) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from mead-baseline[tf2]) (1.15.0)\n",
            "Requirement already satisfied: mead-layers==2.2.3 in /usr/local/lib/python3.6/dist-packages (from mead-baseline[tf2]) (2.2.3)\n",
            "Requirement already satisfied: tensorflow-addons; extra == \"tf2\" in /usr/local/lib/python3.6/dist-packages (from mead-baseline[tf2]) (0.8.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons; extra == \"tf2\"->mead-baseline[tf2]) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NtsNn6Hzc8G"
      },
      "source": [
        "To run our example, we will need to import the `BPEVectorizer1D` which is reponsible for vectorizing text to BPE form, a few utilities from 8-mile including the entire optimizer base and TF packages, as well as the language models we will be using from the `baseline.tf.lm` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg4tATmaB4tx"
      },
      "source": [
        "import time\n",
        "import os\n",
        "from argparse import ArgumentParser\n",
        "import math\n",
        "from typing import Tuple\n",
        "import baseline\n",
        "from eight_mile.utils import str2bool, write_json\n",
        "import baseline.tf.embeddings\n",
        "import baseline.embeddings\n",
        "from baseline.vectorizers import BPEVectorizer1D\n",
        "from eight_mile.utils import Average, get_num_gpus_multiworker, read_yaml\n",
        "from eight_mile.optz import *\n",
        "from eight_mile.tf.optz import *\n",
        "from baseline.tf.lm import SET_TRAIN_FLAG, TransformerLanguageModel, TransformerMaskedLanguageModel\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import json\n",
        "\n",
        "logger = logging.getLogger(\"mead-transformers-tpu\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw-kvOVd0PNI"
      },
      "source": [
        "Here we will define the loss as a function object -- a class with an overloaded `__call__()` function.  We will instantiate using the normal constructor (which takes the BPE `vocab_size` and the context window size), but when its called during optimization, the instance name is called with parens (containing the `model`, `features` and `labels`) as though it was a normal function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZLqR4aGCWY0"
      },
      "source": [
        "\n",
        "class Loss:\n",
        "    def __init__(self, vocab_size, nctx):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.nctx = nctx\n",
        "\n",
        "    def __call__(self, model, features, labels):\n",
        "        logits, _ = model(features, None)\n",
        "        loss_mask = tf.cast(labels != 0, tf.float32)\n",
        "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
        "        losses = losses * loss_mask\n",
        "        losses = tf.reduce_sum(losses)\n",
        "        non_zero = tf.reduce_sum(loss_mask)\n",
        "        losses /= non_zero\n",
        "        return losses"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eaaBALe0wXY"
      },
      "source": [
        "Our preprocessed data files are written in shards of TFRecords (of size ~100MB each) with the feature `x` representing the integer values of the input BPE tokens and `y` representing the target integer values to recover during masked language modeling).\n",
        "\n",
        "Here is an example of what these records would look like as a JSON object:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"x\": [4, 10, 99, 1926, 21, 128, 11, 106, 5, 13288, 33, 7, 5409, 565, 399, 6, 26, 10, 20, 1191, 153, 283, 13, 39, 399, 57, 6, 10, 49, 1616, 5, 63, 1822, 11, 106, 58, 5, 10, 117, 183, 195, 283, 18, 7, 295, 11, 59, 6, 5, 5, 99, 1238, 13, 484, 5, 11723, 852, 18, 2652, 8, 330, 292, 5, 50, 5, 88, 11, 6, 168, 13, 75, 87, 5, 830, 7820, 3012, 18, 834, 9559, 13, 135, 1175, 19, 32, 49, 134, 13, 11, 53, 15, 7, 5939, 1069, 30, 2, 86, 1238, 5, 382, 5129, 8, 4882, 4211, 3383, 42, 5, 19213, 6, 7, 3594, 5, 33, 5129, 5, 221, 17, 140, 8, 38, 5, 49, 127, 19, 7, 11963, 39, 2549, 131, 6, 2, 100, 1238, 13, 39, 28, 18, 2562, 18, 4428, 14327, 7, 44, 12, 1610, 5, 979, 2021, 194, 221, 51, 6, 26, 10, 5, 8432, 7, 3594, 44, 379, 3010, 5, 495, 33, 19, 71, 6, 5, 47, 1713, 174, 16, 644, 15, 231, 7, 3442, 3378, 6, 5, 5, 37, 10, 52, 7, 632, 5, 13, 8, 13, 356, 52, 12, 5, 5, 5, 194, 7, 700, 15, 7, 5, 8, 9311, 28, 1090, 5, 84, 657, 11858, 14043, 8, 9, 294, 27, 28, 37, 63, 6772, 42, 2697, 37, 5, 2488, 19, 31, 347, 21, 7, 971, 41, 5, 2299, 5, 5, 20151, 1645, 37, 11, 66, 294, 5, 18, 7, 5, 15, 7, 5, 17, 233, 19, 92, 56, 13, 5, 86, 5],\n",
        "  \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 10, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 17, 0, 11, 0, 0, 0, 0, 0, 0, 0, 18408, 0, 0, 0, 0, 834, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 15380, 0, 0, 0, 0, 20, 0, 0, 194, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 7, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 83, 127, 0, 0, 0, 0, 0, 6605, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 118, 0, 0, 0, 7, 0, 15, 0, 0, 0, 0, 0, 0, 878, 19, 416, 0, 0, 0, 0, 0, 342, 0, 46, 0, 0, 6002, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3442, 0, 0, 0, 0, 0, 0, 0, 0, 2839, 0, 6, 83, 72, 0, 37, 0, 0, 0, 28, 0, 0, 668, 0, 0, 3378, 0, 0, 0, 0, 0, 0, 30, 0, 644]\n",
        "}\n",
        "```\n",
        "\n",
        "To read the TFRecords, we need a descriptor of these features, and we need to use the [tf.io.parse_single_example()](https://www.tensorflow.org/api_docs/python/tf/io/parse_single_example) function, which we will apply to each record.\n",
        "\n",
        "These files are stored on a GCP bucket, so when we call `get_dataset()` below, we will `glob` the bucket for `*.tfrecord` files then map each record retreival to a pipeline that includes parsing the example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sha4CdqHCb9N"
      },
      "source": [
        "\n",
        "\n",
        "feature_description = {\n",
        "    'x': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True, default_value=0),\n",
        "    'y': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True, default_value=0),\n",
        "}\n",
        "\n",
        "\n",
        "def _parse_tf_record(example_proto):\n",
        "    record = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    return record['x'], record['y']\n",
        "\n",
        "\n",
        "def get_dataset(directory):\n",
        "    pattern = os.path.join(directory, f'*.tfrecord')\n",
        "    files = tf.io.gfile.glob(pattern)\n",
        "    print(files)\n",
        "    ds = tf.data.TFRecordDataset(files).map(_parse_tf_record)\n",
        "    return ds"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXUJzFdL2hPY"
      },
      "source": [
        "Our example will use the `tf.distribute` library, which allows us to provide a [tf.distribute.Strategy](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) for handling distribution details. We just need to create the right sub-class, in this case a [TPUStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy), which we will need to initialize with a [TPUClusterResolver](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver) that takes in an address for the TPU.  Because this example is in colab, we are going to end up calling this function with an empty address and it will use the environment variable `COLAB_TPU_ADDR` to resolve our TPU address."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmK8E16KCmnG"
      },
      "source": [
        "def create_distribute_strategy(strategy_name, endpoint=None):\n",
        "    if strategy_name == 'tpu':\n",
        "        if endpoint is None:\n",
        "            endpoint = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=endpoint)\n",
        "        tf.config.experimental_connect_to_cluster(resolver)\n",
        "        # This is the TPU initialization code that has to be at the beginning.\n",
        "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "    elif strategy_name == 'mirror':\n",
        "        num_gpus = get_num_gpus_multiworker()\n",
        "        devices = ['/device:GPU:{}'.format(i) for i in range(num_gpus)]\n",
        "        strategy = tf.distribute.MirroredStrategy(devices)\n",
        "    else:\n",
        "        raise Exception(f\"Unsupported strategy {strategy_name}\")\n",
        "    return strategy"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKJrbuSx32bi"
      },
      "source": [
        "Now that we have set up a lot of the boilerplate we can go ahead and create our\n",
        "1. `tf.distribute.Strategy` using the function above\n",
        "2. `baseline.Vectorizer` which we will use to initialize the vocabulary of our model\n",
        "3. `baseline.embeddings` whch we will use to initalize a lookup table that projects from our vocabulary size to a hidden unit size (`MLM_MODEL_SZ` below)\n",
        "4. `baseline.tf.lm.TransformerMaskedLanguageModel` which is the model we will be training.  It is built internally on `8 mile` by using an [eight_mile.tf.layers.TransformerEncoderStack](https://github.com/dpressel/mead-baseline/blob/master/layers/eight_mile/tf/layers.py), which composes a stack of `Transformer` encoders with interleaved multi-head attention and FFN sub-stacks.\n",
        "\n",
        "The `TransformerMaskedLanguageModel` is a type of `class AbstractGeneratorModel(LanguageModelBase)`, which defines the abstract phases to create a `LanguageModel`:\n",
        "\n",
        "```python\n",
        "    def create_layers(self, embeddings, **kwargs):\n",
        "        self.embeddings = self.init_embed(embeddings, **kwargs)\n",
        "        self.embeddings_proj = self.init_embeddings_proj(**kwargs)\n",
        "        self.generator = self.init_generate(**kwargs)\n",
        "        self.output_layer = self.init_output(embeddings, **kwargs)\n",
        "```\n",
        "\n",
        "This gets sub-classed by the `TransformerLanguageModel` to provide a `Transformer`-based `generator` object:\n",
        "\n",
        "```python\n",
        "    def init_generate(self, **kwargs):\n",
        "        pdrop = float(kwargs.get('dropout', 0.1))\n",
        "        layers = kwargs.get('layers', kwargs.get('num_layers', 1))\n",
        "        d_model = int(kwargs.get('d_model', kwargs.get('hsz')))\n",
        "        num_heads = kwargs.get('num_heads', 4)\n",
        "        d_ff = int(kwargs.get('d_ff', 4 * d_model))\n",
        "        rpr_k = kwargs.get('rpr_k')\n",
        "        d_k = kwargs.get('d_k')\n",
        "        scale = bool(kwargs.get('scale', True))\n",
        "        activation = kwargs.get('activation', 'gelu')\n",
        "        layer_norm_eps = kwargs.get('layer_norm_eps', 1e-12)\n",
        "        layer_norms_after = kwargs.get('layer_norms_after', False)\n",
        "        return TransformerEncoderStack(num_heads, d_model=d_model, pdrop=pdrop, scale=scale,\n",
        "                                       layers=layers, d_ff=d_ff, rpr_k=rpr_k, d_k=d_k,\n",
        "                                       activation=activation, layer_norm_eps=layer_norm_eps,\n",
        "                                       layer_norms_after=layer_norms_after)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aioXP-exCsFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c8d55d-31be-40c7-9166-a065c0ab0204"
      },
      "source": [
        "\n",
        "\n",
        "MLM_MODEL_SZ = 512\n",
        "MLM_FFN_SZ = 4 * MLM_MODEL_SZ\n",
        "MLM_CONTEXT_LENGTH = 256\n",
        "MLM_REL_POS_REPR = 8\n",
        "MLM_DROPOUT = 0.1\n",
        "MLM_NUM_HEADS = 8\n",
        "MLM_NUM_LAYERS = 8\n",
        "SET_TRAIN_FLAG(True)\n",
        "\n",
        "SUBWORD_MODEL_FILE = './wt103/codes.30k'\n",
        "SUBWORD_VOCAB_FILE = './wt103/vocab.30k'\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "strategy = create_distribute_strategy(\"tpu\")\n",
        "num_replicas = strategy.num_replicas_in_sync\n",
        "logger.info(f\"Using {num_replicas} replicas in this job.\")\n",
        "vectorizer = BPEVectorizer1D(model_file=SUBWORD_MODEL_FILE,\n",
        "                             vocab_file=SUBWORD_VOCAB_FILE,\n",
        "                             mxlen=MLM_MODEL_SZ)\n",
        "vocab = {'x': vectorizer.vocab}\n",
        "preproc_data = baseline.embeddings.load_embeddings('x',\n",
        "                                                   dsz=MLM_MODEL_SZ,\n",
        "                                                   known_vocab=vocab['x'],\n",
        "                                                   preserve_vocab_indices=True,\n",
        "                                                   embed_type=\"default\")\n",
        "vocabs = preproc_data['vocab']\n",
        "vocab_size = max(vocabs.values())\n",
        "embeddings = {'x': preproc_data['embeddings']}\n",
        "\n",
        "model = TransformerMaskedLanguageModel.create(embeddings,\n",
        "                                              hsz=MLM_MODEL_SZ,\n",
        "                                              d_ff=MLM_FFN_SZ,\n",
        "                                              tie_weights=True,\n",
        "                                              dropout=MLM_DROPOUT,\n",
        "                                              num_heads=MLM_NUM_HEADS,\n",
        "                                              layers=MLM_NUM_LAYERS,\n",
        "                                              rpr_k=MLM_REL_POS_REPR,\n",
        "                                              src_keys=['x'], tgt_key='x')\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.23.118.226:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.23.118.226:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.23.118.226:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.23.118.226:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n",
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "INFO:mead-transformers-tpu:Using 8 replicas in this job.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4bSuLW26L0D"
      },
      "source": [
        "We have defined our model and our loss function, as well as the initial step of our data pipeline using `TFRecordDataset`.  Now we will provide functions that will distribute our reader over each replica using the `strategy.experimental_distribute_datasets_from_function` API call, and batches the dataset into our per-replica dataset size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hxAutT7EVCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60680122-0338-49fb-c683-35df6734f500"
      },
      "source": [
        "BATCH_SIZE = 8 * 20\n",
        "TRAIN_BUCKET = \"gs://lm-sample/wt103/train\"\n",
        "VALID_BUCKET = \"gs://lm-sample/wt103/valid\"\n",
        "MD_TRAIN_FILE = \"./wt103/train/md.yml\"\n",
        "MD_TEST_FILE = \"./wt103/valid/md.yml\"\n",
        "\n",
        "def get_num_samples(f):\n",
        "    yml = read_yaml(f)\n",
        "    return yml['num_samples']\n",
        "\n",
        "def dataset_train_fn(input_context):\n",
        "    batch_size = input_context.get_per_replica_batch_size(BATCH_SIZE)\n",
        "    ds = get_dataset(TRAIN_BUCKET).batch(batch_size)\n",
        "    return ds.shard(\n",
        "        input_context.num_input_pipelines, input_context.input_pipeline_id\n",
        "    )\n",
        "train_loader = strategy.experimental_distribute_datasets_from_function(dataset_train_fn)\n",
        "\n",
        "def dataset_test_fn(input_context):\n",
        "    batch_size = input_context.get_per_replica_batch_size(BATCH_SIZE)\n",
        "    ds = get_dataset(VALID_BUCKET).batch(batch_size)\n",
        "    return ds.shard(\n",
        "        input_context.num_input_pipelines, input_context.input_pipeline_id\n",
        "    )\n",
        "valid_loader = strategy.experimental_distribute_datasets_from_function(dataset_test_fn)\n",
        "\n",
        "num_train_samples = get_num_samples(MD_TRAIN_FILE)\n",
        "num_valid_samples = get_num_samples(MD_TEST_FILE)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gs://lm-sample/wt103/train/train-1.tfrecord', 'gs://lm-sample/wt103/train/train-2.tfrecord', 'gs://lm-sample/wt103/train/train-3.tfrecord']\n",
            "['gs://lm-sample/wt103/valid/valid-1.tfrecord']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_X1YPjV69pu"
      },
      "source": [
        "Now we can create our loss function, and set some variables so we can get periodic updates while we train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WumPrXLvGDlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea92f57-88f5-4c63-bcd2-a768cc8775d5"
      },
      "source": [
        "loss_function = Loss(vocab_size, MLM_CONTEXT_LENGTH)\n",
        "steps_per_epoch = num_train_samples // BATCH_SIZE\n",
        "steps_per_valid_epoch = num_valid_samples // BATCH_SIZE\n",
        "update_on = steps_per_epoch // 10\n",
        "report_on = update_on // 10\n",
        "logger.info(f\"Steps per epoch: {steps_per_epoch}. Update every {update_on} steps.\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:mead-transformers-tpu:Steps per epoch: 2711. Update every 271 steps.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFGrzt3t7J05"
      },
      "source": [
        "The `8 mile` API provides an `EagerOptimizer` which takes in a loss function (or function object in our case) and applies it for every step of training to provide a per-replica loss.  We are going to create a learning regimen that starts with a linear warmup to the target learning rate over `WARMUP_STEPS` composed with learning rate scheduler that provides a cosine decay.\n",
        "\n",
        "We will specify that our model is to be trained with [adamw](https://www.fast.ai/2018/07/02/adam-weight-decay/) (we use the `tensorflow_addons` implementation underneath) with weight decay given by `WD` below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMhh55X9IMCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840c6d3d-c7bb-4814-c3d2-5c2032ac72cb"
      },
      "source": [
        "LR = 4.0e-4\n",
        "EPOCHS = 1\n",
        "GRAD_CLIP = 1.0\n",
        "OPTIM = \"adamw\"\n",
        "WD = 1.0e-2\n",
        "WARMUP_STEPS = 10000\n",
        "lr_decay = CosineDecaySchedulerTensorFlow(steps_per_epoch * EPOCHS, lr=LR)\n",
        "linear_warmup = WarmupLinearSchedulerTensorFlow(WARMUP_STEPS, lr=LR)\n",
        "lr_sched = CompositeLRSchedulerTensorFlow(linear_warmup, lr_decay)\n",
        "\n",
        "optimizer = EagerOptimizer(loss_function, global_step=1, lr=LR, optim=OPTIM,\n",
        "                           lr_function=lr_sched,\n",
        "                           weight_decay=WD, clip=GRAD_CLIP)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:mead.layers:adamw(eta=0.000400 beta1=0.900000, beta2=0.999000, eps=0.000000, wd=0.010000)\n",
            "INFO:mead.layers:clip gradients at 1.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Krt3L58JII"
      },
      "source": [
        "We are going to define 2 autograph compiled functions, one for distributed training and one for distributed testing.  The implementations proxy to an underlying per-replica function which optimize and provide back the per-replica losses, which are accumulated in the distributed function.  These functions use the `strategy.run` function to call the per-replica versions, and `strategy.reduce` to sum the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLrkWby-LcqD"
      },
      "source": [
        "\n",
        "def _replicated_train_step(inputs):\n",
        "    x, y = inputs\n",
        "    per_replica_loss = optimizer.update(model, {'x': x}, y, num_replicas)\n",
        "    return per_replica_loss\n",
        "\n",
        "@tf.function\n",
        "def _distributed_train_step(inputs: Tuple[tf.Tensor, tf.Tensor]):\n",
        "    per_replica_loss = strategy.run(_replicated_train_step, args=(inputs,))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
        "\n",
        "valid_loss_function = Loss(vocab_size, MLM_CONTEXT_LENGTH)\n",
        "\n",
        "def _replicated_test_step(inputs):\n",
        "    x, y = inputs\n",
        "    per_replica_loss = valid_loss_function(model, {'x': x}, y) / num_replicas\n",
        "    return per_replica_loss\n",
        "\n",
        "@tf.function\n",
        "def _distributed_test_step(inputs: Tuple[tf.Tensor, tf.Tensor]):\n",
        "    per_replica_loss = strategy.run(_replicated_test_step, args=(inputs,))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFZ56p-B8oIx"
      },
      "source": [
        "We have finally set up all the boilerplate and we can run a normal training loop.  The only difference for a distributed program within the training loop is that we call the operations within a `strategy.scope()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMTTpSP_MCHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ce3fd1-3b83-4ade-d28d-495c1c8478d5"
      },
      "source": [
        "start_epoch = 0\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "    for epoch in range(start_epoch, EPOCHS):\n",
        "        SET_TRAIN_FLAG(True)\n",
        "        avg_loss = Average('average_train_loss')\n",
        "        metrics = {}\n",
        "        start = time.time()\n",
        "        train_iter = iter(train_loader)\n",
        "        for i in range(steps_per_epoch):\n",
        "            loss = _distributed_train_step(next(train_iter))\n",
        "            avg_loss.update(loss.numpy().item())\n",
        "            if (i + 1) % report_on == 0:\n",
        "                logging.info(avg_loss)\n",
        "            if (i + 1) % update_on == 0:\n",
        "                elapsed = (time.time() - start)/60\n",
        "                logging.info('elapsed time this epoch %d min', elapsed)\n",
        "                logging.info('elapsed step time %f steps/min', i/elapsed)\n",
        "\n",
        "        # How much time elapsed in minutes\n",
        "        elapsed = (time.time() - start)/60\n",
        "        train_token_loss = avg_loss.avg\n",
        "        # This is the average training token-level loss across all machines\n",
        "        # This is the token-level training perplexity\n",
        "        train_token_ppl = math.exp(train_token_loss)\n",
        "        metrics['train_elapsed_min'] = elapsed\n",
        "        metrics['average_train_loss'] = train_token_loss\n",
        "        metrics['train_ppl'] = train_token_ppl\n",
        "        metrics['lr'] = float(lr_sched(tf.cast(optimizer.global_step, tf.float32)).numpy().item())\n",
        "\n",
        "        avg_valid_loss = Average('average_valid_loss')\n",
        "        start = time.time()\n",
        "        SET_TRAIN_FLAG(False)\n",
        "        valid_iter = iter(valid_loader)\n",
        "        for i in range(steps_per_valid_epoch):\n",
        "            valid_loss = _distributed_test_step(next(valid_iter))\n",
        "            avg_valid_loss.update(valid_loss.numpy().item())\n",
        "\n",
        "        valid_token_loss = avg_valid_loss.avg\n",
        "        valid_token_ppl = math.exp(valid_token_loss)\n",
        "\n",
        "        elapsed = (time.time() - start)/60\n",
        "        metrics['valid_elapsed_min'] = elapsed\n",
        "        metrics['average_valid_loss'] = valid_token_loss\n",
        "        metrics['average_valid_token_ppl'] = valid_token_ppl\n",
        "        print(json.dumps(metrics, indent=4, sort_keys=True))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:average_train_loss 10.553317 (10.756089)\n",
            "INFO:root:average_train_loss 9.598961 (10.413898)\n",
            "INFO:root:average_train_loss 9.032587 (10.035762)\n",
            "INFO:root:average_train_loss 8.658846 (9.743754)\n",
            "INFO:root:average_train_loss 8.604391 (9.522078)\n",
            "INFO:root:average_train_loss 8.343370 (9.344612)\n",
            "INFO:root:average_train_loss 8.118359 (9.195230)\n",
            "INFO:root:average_train_loss 8.064242 (9.068246)\n",
            "INFO:root:average_train_loss 7.967513 (8.954723)\n",
            "INFO:root:average_train_loss 7.858014 (8.851719)\n",
            "INFO:root:elapsed time this epoch 2 min\n",
            "INFO:root:elapsed step time 121.339746 steps/min\n",
            "INFO:root:average_train_loss 7.733735 (8.758462)\n",
            "INFO:root:average_train_loss 7.761401 (8.673860)\n",
            "INFO:root:average_train_loss 7.600109 (8.594115)\n",
            "INFO:root:average_train_loss 7.420698 (8.516933)\n",
            "INFO:root:average_train_loss 7.329846 (8.446820)\n",
            "INFO:root:average_train_loss 7.387989 (8.380190)\n",
            "INFO:root:average_train_loss 7.222541 (8.316434)\n",
            "INFO:root:average_train_loss 7.120338 (8.256194)\n",
            "INFO:root:average_train_loss 7.111787 (8.197552)\n",
            "INFO:root:average_train_loss 7.039405 (8.143068)\n",
            "INFO:root:elapsed time this epoch 3 min\n",
            "INFO:root:elapsed step time 169.038169 steps/min\n",
            "INFO:root:average_train_loss 7.047856 (8.089414)\n",
            "INFO:root:average_train_loss 6.931404 (8.039394)\n",
            "INFO:root:average_train_loss 6.989418 (7.992501)\n",
            "INFO:root:average_train_loss 6.877005 (7.947525)\n",
            "INFO:root:average_train_loss 6.880800 (7.904437)\n",
            "INFO:root:average_train_loss 6.864063 (7.863539)\n",
            "INFO:root:average_train_loss 6.814633 (7.824425)\n",
            "INFO:root:average_train_loss 6.753318 (7.787646)\n",
            "INFO:root:average_train_loss 6.885804 (7.752914)\n",
            "INFO:root:average_train_loss 6.759607 (7.720633)\n",
            "INFO:root:elapsed time this epoch 4 min\n",
            "INFO:root:elapsed step time 194.149294 steps/min\n",
            "INFO:root:average_train_loss 6.705241 (7.689406)\n",
            "INFO:root:average_train_loss 6.772897 (7.659775)\n",
            "INFO:root:average_train_loss 6.677691 (7.630590)\n",
            "INFO:root:average_train_loss 6.716178 (7.603003)\n",
            "INFO:root:average_train_loss 6.841704 (7.577839)\n",
            "INFO:root:average_train_loss 6.772398 (7.553442)\n",
            "INFO:root:average_train_loss 6.722457 (7.529772)\n",
            "INFO:root:average_train_loss 6.549517 (7.507177)\n",
            "INFO:root:average_train_loss 6.558844 (7.484914)\n",
            "INFO:root:average_train_loss 6.730065 (7.464347)\n",
            "INFO:root:elapsed time this epoch 5 min\n",
            "INFO:root:elapsed step time 209.138515 steps/min\n",
            "INFO:root:average_train_loss 6.638474 (7.444075)\n",
            "INFO:root:average_train_loss 6.632908 (7.425223)\n",
            "INFO:root:average_train_loss 6.587750 (7.407350)\n",
            "INFO:root:average_train_loss 6.591558 (7.389657)\n",
            "INFO:root:average_train_loss 6.685539 (7.372743)\n",
            "INFO:root:average_train_loss 6.653189 (7.356898)\n",
            "INFO:root:average_train_loss 6.618579 (7.341667)\n",
            "INFO:root:average_train_loss 6.580826 (7.326705)\n",
            "INFO:root:average_train_loss 6.577073 (7.311798)\n",
            "INFO:root:average_train_loss 6.677652 (7.298034)\n",
            "INFO:root:elapsed time this epoch 6 min\n",
            "INFO:root:elapsed step time 219.791451 steps/min\n",
            "INFO:root:average_train_loss 6.693425 (7.284708)\n",
            "INFO:root:average_train_loss 6.571055 (7.271955)\n",
            "INFO:root:average_train_loss 6.643128 (7.258975)\n",
            "INFO:root:average_train_loss 6.612744 (7.246610)\n",
            "INFO:root:average_train_loss 6.563533 (7.234314)\n",
            "INFO:root:average_train_loss 6.541025 (7.222576)\n",
            "INFO:root:average_train_loss 6.413618 (7.211428)\n",
            "INFO:root:average_train_loss 6.516551 (7.200656)\n",
            "INFO:root:average_train_loss 6.452726 (7.189747)\n",
            "INFO:root:average_train_loss 6.733402 (7.178997)\n",
            "INFO:root:elapsed time this epoch 7 min\n",
            "INFO:root:elapsed step time 227.437552 steps/min\n",
            "INFO:root:average_train_loss 6.481847 (7.167921)\n",
            "INFO:root:average_train_loss 6.434821 (7.157027)\n",
            "INFO:root:average_train_loss 6.341182 (7.146308)\n",
            "INFO:root:average_train_loss 6.448914 (7.135443)\n",
            "INFO:root:average_train_loss 6.363413 (7.124412)\n",
            "INFO:root:average_train_loss 6.386326 (7.112977)\n",
            "INFO:root:average_train_loss 6.313480 (7.101871)\n",
            "INFO:root:average_train_loss 6.241085 (7.090231)\n",
            "INFO:root:average_train_loss 6.409204 (7.078627)\n",
            "INFO:root:average_train_loss 6.168229 (7.066564)\n",
            "INFO:root:elapsed time this epoch 8 min\n",
            "INFO:root:elapsed step time 232.667031 steps/min\n",
            "INFO:root:average_train_loss 6.072540 (7.055084)\n",
            "INFO:root:average_train_loss 6.233415 (7.043148)\n",
            "INFO:root:average_train_loss 6.122707 (7.031480)\n",
            "INFO:root:average_train_loss 6.067902 (7.019387)\n",
            "INFO:root:average_train_loss 6.100300 (7.007494)\n",
            "INFO:root:average_train_loss 6.168234 (6.995233)\n",
            "INFO:root:average_train_loss 6.116827 (6.983082)\n",
            "INFO:root:average_train_loss 5.969505 (6.970265)\n",
            "INFO:root:average_train_loss 6.092281 (6.957819)\n",
            "INFO:root:average_train_loss 5.896419 (6.945188)\n",
            "INFO:root:elapsed time this epoch 9 min\n",
            "INFO:root:elapsed step time 237.261073 steps/min\n",
            "INFO:root:average_train_loss 5.828893 (6.932314)\n",
            "INFO:root:average_train_loss 5.918633 (6.919229)\n",
            "INFO:root:average_train_loss 5.686731 (6.906170)\n",
            "INFO:root:average_train_loss 5.841617 (6.893071)\n",
            "INFO:root:average_train_loss 5.697813 (6.880343)\n",
            "INFO:root:average_train_loss 5.858001 (6.866749)\n",
            "INFO:root:average_train_loss 5.627368 (6.853501)\n",
            "INFO:root:average_train_loss 5.831038 (6.840560)\n",
            "INFO:root:average_train_loss 5.774557 (6.827175)\n",
            "INFO:root:average_train_loss 5.739478 (6.813950)\n",
            "INFO:root:elapsed time this epoch 10 min\n",
            "INFO:root:elapsed step time 240.954679 steps/min\n",
            "INFO:root:average_train_loss 5.606785 (6.800543)\n",
            "INFO:root:average_train_loss 5.685167 (6.787362)\n",
            "INFO:root:average_train_loss 5.407146 (6.774118)\n",
            "INFO:root:average_train_loss 5.620404 (6.760773)\n",
            "INFO:root:average_train_loss 5.371620 (6.747378)\n",
            "INFO:root:average_train_loss 5.315873 (6.733951)\n",
            "INFO:root:average_train_loss 5.251081 (6.720116)\n",
            "INFO:root:average_train_loss 5.295675 (6.706692)\n",
            "INFO:root:average_train_loss 5.475350 (6.693330)\n",
            "INFO:root:average_train_loss 5.334508 (6.680181)\n",
            "INFO:root:elapsed time this epoch 11 min\n",
            "INFO:root:elapsed step time 243.508825 steps/min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"average_train_loss\": 6.67507763605459,\n",
            "    \"average_valid_loss\": 5.111191654205323,\n",
            "    \"average_valid_token_ppl\": 165.8678943247949,\n",
            "    \"lr\": 0.00010844000644283369,\n",
            "    \"train_elapsed_min\": 11.128791542847951,\n",
            "    \"train_ppl\": 792.4089709935873,\n",
            "    \"valid_elapsed_min\": 0.13558365106582643\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0cE-Ije8-xY"
      },
      "source": [
        "Wrap-Up and Next Steps\n",
        "\n",
        "In this example, we used the MEAD/Baseline API code to train a Transformer Language Model on TPUs.  To keep things very simple, we did not write out any checkpoints or any training logs.  To see how you would do this with the `CheckpointManager`, see the [full API example](https://github.com/dpressel/mead-baseline/blob/master/api-examples/pretrain_tlm_tf.py).\n",
        "\n",
        "Also we only trained this for a single epoch.  For a compelling MLM you probably would want to run this for many epochs."
      ]
    }
  ]
}