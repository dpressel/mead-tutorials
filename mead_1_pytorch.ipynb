{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mead-1-pytorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dpressel/mead-tutorials/blob/master/mead_1_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ObdkwD8V0cb",
        "colab_type": "text"
      },
      "source": [
        "# Getting Started with MEAD Using PyTorch\n",
        "\n",
        "In this example, we will download and test drive [mead-baseline](https://github.com/dpressel/mead-baseline) using the PyTorch backend. Then we will show how use a custom model as an addon.  Its recommended to be familiar with the basic problems we are trying to solve here, which are discussed in detail in my tutorial from the [Deep Learning Summer School Tutorial from Gdansk, 2019](https://github.com/dpressel/dliss-tutorial), especially as background for the implementation details.\n",
        "\n",
        "This document is written in Colab and uses free GPUs from Google, which is really awesome, but it also means that the examples will run at a fraction of the speed of my laptop with its RTX 2080 or even a GTX 1070.  Particularly, our fine-tuning BERT example at the end will run particularly slowly.  Later in the tutorial, I will provide an alternate configuration that will run much faster\n",
        "\n",
        "To get started, the first thing we need to do is install the latest version of the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsubIBXXrozS",
        "colab_type": "code",
        "outputId": "686d986c-cfa3-42fc-87f3-f79682d3cd6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!pip install wheel\n",
        "!pip install torch\n",
        "!pip install mead-baseline[yaml]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wheel in /usr/local/lib/python3.6/dist-packages (0.34.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Collecting mead-baseline[yaml]\n",
            "  Using cached https://files.pythonhosted.org/packages/7d/bf/42b54c0d418341fdb81acc3c8e661bc6cae98d5650594292b7502d559456/mead_baseline-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: mead-layers in /usr/local/lib/python3.6/dist-packages (from mead-baseline[yaml]) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mead-baseline[yaml]) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from mead-baseline[yaml]) (1.12.0)\n",
            "Requirement already satisfied: pyyaml; extra == \"yaml\" in /usr/local/lib/python3.6/dist-packages (from mead-baseline[yaml]) (3.13)\n",
            "Installing collected packages: mead-baseline\n",
            "Successfully installed mead-baseline-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVyy90xxXONp",
        "colab_type": "text"
      },
      "source": [
        "The `mead-baseline` project has very dependencies.  The basic install requires only\n",
        "\n",
        "- `numpy`\n",
        "- `six`\n",
        "- `torch` or `tensorflow`\n",
        "\n",
        "Above, we installed with an optional `YAML` dependency.  If you do not install this dependency on your system, the program will run just fine, but will only accept JSON configuration files.  For our purposes in this tutorial, `YAML` is a little easier to digest.\n",
        "\n",
        "For starters, we are just going to try to train a basic model using a YAML configuration file.  The example we will use is [from the repository](https://github.com/dpressel/mead-baseline/blob/master/mead/config/sst2-pyt.yml).  We will break it down and describe it piece-by-piece here:\n",
        "\n",
        "```yaml\n",
        "task: classify\n",
        "basedir: sst2-pyt\n",
        "preproc:\n",
        "  mxlen: 100\n",
        "  clean: true\n",
        "backend: pytorch\n",
        "dataset: SST2\n",
        "unif: 0.25\n",
        "```\n",
        "\n",
        "The `basedir` param stores our output.  Below that is a an optional \"block\" for preprocessing.  This is an optional way of defining overarching pre-processing params that should apply to the data globally.  In future examples we will see that these same transformations can be applied at the vectorization layer which is a bit cleaner.\n",
        "\n",
        "The `mxlen` argument tell the training program that we wish to enforce a hard limit on the length of any features in the program.  By default, the maximum length of the features are determined in pre-processing by the maximum length observed in the data, but in this example `100` is more than adequate.\n",
        "\n",
        "The `clean` option tells the program to apply data-specific transformations to clean up the input.  As a general rule, this should not be applied -- the best practice in MEAD is to have fully pre-processed the data ahead of time.  However, in a few cases, such as for this dataset, published work has applied a fairly standard set of transformations that we wish to recreate after downloading the original data files.\n",
        "\n",
        "The `backend` parameter tells `mead-baseline` what backend we are using.  It can also be supplied or overridden in the command-line.  So for example, even though this configuration specifies its default `backend` as `pytorch`, we can override this at the command-line by providing an `--backend tf` option.\n",
        "\n",
        "The `unif` option is also supported with local scoping within each feature, which we can see later -- here we are just being lazy and using the global property to override any and all features to tell them to initialize random weights between `-0.25` and `0.25`.\n",
        "\n",
        "```yaml\n",
        "loader:\n",
        "  reader_type: default\n",
        "```\n",
        "\n",
        "The block above can be titled `loader` or `reader`, they are interchangeable.  The `reader_type` here is given as `default`.  The software has a registered `default` handler for each task and, in the case of classification, the [reader](https://github.com/dpressel/mead-baseline/blob/master/baseline/reader.py) is defined as a tab-separated value reader where the first column is the label, and the second column is some text.\n",
        "\n",
        "You might have more complicated classification features, in which case, you might override this default with your own reader that can provide a set of features.  However, we are doing deep-learning and one of its supposed strengths is its ability to learn good feature representations from (nearly) raw input, so our single input reader should be sufficient for many examples.\n",
        "\n",
        "```yaml\n",
        "model:\n",
        "  model_type: default\n",
        "  filtsz: [3,4,5]\n",
        "  cmotsz: 100\n",
        "  dropout: 0.5\n",
        "```\n",
        "\n",
        "The `model` block describes our basic model archicture.  Our `default` model for classification is a parallel-filter convolutional neural network with a max-over-time pooling applied to the convolutional output.\n",
        "\n",
        "This model in described in more detail and along with a from-scratch implementation in my [Deep Learning Summer School Tutorial from Gdansk, 2019](https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/1_pretrained_vectors.ipynb)\n",
        "\n",
        "\n",
        "```yaml\n",
        "features:\n",
        "  - name: word\n",
        "    vectorizer:\n",
        "      type: token1d\n",
        "      transform: baseline.lowercase\n",
        "    embeddings:\n",
        "      label: w2v-gn\n",
        "```\n",
        "A strength of `mead-baseline` is its ability to incorporate multiple features into the model.  These features may be different representations of the same surface text, with optionally different pre-processing, or they may incorporate different features from the input altogether (like the Part-of-Speech tags).\n",
        "\n",
        "Each feature in `mead-baseline` is incorporated into the model with a corresponding `embedding` which provides a dense representation of the feature (`embeddings` are also covered in my talk listed above).\n",
        "\n",
        "The `default` embeddings are just basic word or character vectors (anything that can be instantiated with a lookup table).   Additionally, MEAD supports a lot of different pre-trained word embeddings out of the box, including `GloVe`, `Word2Vec` and `fastText`, as well as just random initializations for each word.  In the example above, we use a `label: w2v-gn` to indicate that we want the training program to look for a key named `w2v-gn` inside its `embeddings` \"index\" and access or download those embeddings and use them.\n",
        "\n",
        "You might be wondering where this index is provided.  It turns out that this is an optional argument to our trainer program [mead-train](https://github.com/dpressel/mead-baseline/blob/master/mead/trainer.py), and that it defaults to an installed index that does contain this entry (among others):\n",
        "\n",
        "```json\n",
        " {\n",
        "    \"label\": \"w2v-gn\",\n",
        "    \"file\": \"https://www.dropbox.com/s/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz?dl=1\",\n",
        "    \"dsz\": 300\n",
        "  },\n",
        "```\n",
        "The [default embeddings index](https://github.com/dpressel/mead-baseline/blob/master/mead/config/embeddings.json) is installed when we install the package as a local file, or it can be expressed as a URL (using local files in the default installed embeddings index would have little utility, so all of the examples in the installed `config/embeddings.json` use URLs and download the embeddings to a local cache (which default to `~/.bl-data`).  The `dsz` parameter above tells us the hidden (or embedding) dimensionality of the vectors (in this case, `300`).  This means if you look for the word `egg` in the our word2vec embeddings above, you will get back a 300-dimensional tensor.\n",
        "\n",
        "```yaml\n",
        "train:\n",
        "  batchsz: 50\n",
        "  epochs: 2\n",
        "  optim: adadelta\n",
        "  eta: 1.0\n",
        "  early_stopping_metric: acc\n",
        "  verbose:\n",
        "    console: True\n",
        "    file: sst2-cm.csv\n",
        "```\n",
        "The `train` block above defines key training information like the batch size (`batchsz`), the number of times to iterate the full training set (`epochs`), and the optimizer (`optim`) and its learning rate (`lr` or `eta` can be used interchangeably).  Most commonly defined optimizers are supported.\n",
        "The `early_stopping_metric`, along with the `do_early_stopping` boolean (which defaults to `true`) defines if we should do early stopping on the dataset using the validation set and, if so, what metric to use -- here `acc`.  Once training is completed, the best performing epoch in terms of accuracy will be persisted and used for final testing (and future inference).\n",
        "\n",
        "*Note*: when you have `mead-baseline` installed on your local machine, its very common to refer to a local file using the `--config` option:\n",
        "\n",
        "```\n",
        "mead-train --config config/sst2.json\n",
        "```\n",
        "\n",
        "For convenience and to keep the examples up-to-date, we will provide our driver program with a URL instead for the configuration location, and we will do this consistently throughout the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoVDLd3DekhA",
        "colab_type": "code",
        "outputId": "4a58f532-6ae9-43f5-8ff8-5d2885dfd217",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!mead-train --config https://raw.githubusercontent.com/dpressel/mead-baseline/master/mead/config/sst2-pyt.yml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading config file '/tmp/tmpg2la4mq1'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/logging.json'\n",
            "No file found '/usr/local/l...', loading as string\n",
            "\u001b[33;1mWarning: no mead-settings file was found at [/usr/local/lib/python3.6/dist-packages/mead/config/mead-settings.json]\u001b[0m\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/datasets.json'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/embeddings.json'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/vecs.json'\n",
            "Task: [classify]\n",
            "using /root/.bl-data as data/embeddings cache\n",
            "Clean\n",
            "Progress: [========================================] 100%\n",
            "extracting file..\n",
            "downloaded data saved in /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917\n",
            "[train file]: /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917/stsa.binary.phrases.train\n",
            "[valid file]: /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917/stsa.binary.dev\n",
            "[test file]: /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917/stsa.binary.test\n",
            "Progress: [========================================] 100%\n",
            "tcmalloc: large alloc 1743568896 bytes == 0x3f88000 @  0x7f363e3261e7 0x59203c 0x4ca610 0x56697a 0x5a4be1 0x5a5cda 0x4ce182 0x50a2bf 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50cd96 0x509758 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091\n",
            "extracting file..\n",
            "downloaded data saved in /root/.bl-data/281bc75825fa6474e95a1de715f49a3b4e153822\n",
            "embedding file location: /root/.bl-data/281bc75825fa6474e95a1de715f49a3b4e153822\n",
            "model file [sst2-pyt/classify-model-1055.pyt]\n",
            "Doing early stopping on [acc] with patience [2]\n",
            "reporting [<bound method EpochReportingHook.step of <baseline.reporting.LoggingReporting object at 0x7f3636581cf8>>]\n",
            "Calling model <function register_model.<locals>.create at 0x7f35e7ed0a60>\n",
            "ConvModel(\n",
            "  (embeddings): EmbeddingsStack(\n",
            "    (embeddings): ModuleList(\n",
            "      (0): LookupTableEmbeddingsModel(\n",
            "        (embeddings): Embedding(17241, 300, padding_idx=0)\n",
            "      )\n",
            "    )\n",
            "    (reduction): ConcatReduction()\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (pool_model): WithoutLength(\n",
            "    (layer): WithDropout(\n",
            "      (layer): ParallelConv(\n",
            "        (convs): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "            (1): ReLU()\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Conv1d(300, 100, kernel_size=(4,), stride=(1,), padding=(2,))\n",
            "            (1): ReLU()\n",
            "          )\n",
            "          (2): Sequential(\n",
            "            (0): Conv1d(300, 100, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "            (1): ReLU()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (dropout): Dropout(p=0.5, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (stack_model): PassThru()\n",
            "  (output_layer): Dense(\n",
            "    (layer): Linear(in_features=300, out_features=2, bias=True)\n",
            "    (activation): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "adadelta(eta=1.000000, wd=0.000000)\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 1, \"phase\": \"Train\", \"acc\": 0.8741570405789946, \"precision\": 0.8836929890689785, \"recall\": 0.8876452353344849, \"f1\": 0.8856647030351682, \"mcc\": 0.745749115667226, \"avg_loss\": 0.3017748598651149}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 1, \"phase\": \"Valid\", \"acc\": 0.8520642201834863, \"precision\": 0.8795180722891566, \"recall\": 0.8220720720720721, \"f1\": 0.8498253783469151, \"mcc\": 0.7059502488363372, \"avg_loss\": 0.36534401815418804}\n",
            "New best 0.852\n",
            "saving sst2-pyt/classify-model-1055.pyt\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 2, \"phase\": \"Train\", \"acc\": 0.9238445446394927, \"precision\": 0.9292824456290985, \"recall\": 0.932251118105019, \"f1\": 0.9307644147283616, \"mcc\": 0.8461580415575388, \"avg_loss\": 0.20400675952552066}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 2, \"phase\": \"Valid\", \"acc\": 0.8463302752293578, \"precision\": 0.8954081632653061, \"recall\": 0.7905405405405406, \"f1\": 0.8397129186602871, \"mcc\": 0.6981936022002624, \"avg_loss\": 0.40957426918892686}\n",
            "Best performance on acc: 0.852 at epoch 0\n",
            "Reloading best checkpoint\n",
            "adadelta(eta=1.000000, wd=0.000000)\n",
            "Progress: [========================================] 100%\n",
            "                 0        1\n",
            "        0      823       89\n",
            "        1      133      776\n",
            "\n",
            "\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 0, \"phase\": \"Test\", \"acc\": 0.8780889621087314, \"precision\": 0.8971098265895954, \"recall\": 0.8536853685368537, \"f1\": 0.874859075535513, \"mcc\": 0.7570424781231607, \"avg_loss\": 0.31000091771317473}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSrxX8sakGfZ",
        "colab_type": "text"
      },
      "source": [
        "To recap, the `mead-train` program trained our deep learning model on the SST2 dataset for 2 epochs, using the `adadelta` optimizer, following [Kim 2014](http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf), and we can see that our results successfully replicate the performance in that work.\n",
        "\n",
        "The `mead-baseline` package provides us with a few different options for builtin baselines.  What if we wanted to run the same dataset, using the LSTM model described and implemented in my [Deep Learning Summer School Tutorial from Gdansk, 2019](https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/1_pretrained_vectors.ipynb)?\n",
        "\n",
        "To do this, we really dont need to change much!  In fact, while we are at it, lets add a few more feature representations!  First, our model updates:\n",
        "\n",
        "```yaml\n",
        "model:\n",
        "  model_type: lstm\n",
        "  rnnsz: 100\n",
        "  dropout: 0.5\n",
        "```\n",
        "Not much changed here.  We are telling it to use our built-in `lstm` classifier, with a hidden size (`rnnsz`) for the LSTM of `100`.\n",
        "\n",
        "Now lets make the features section a little more interesting.  Here we are going to stack some pretrained embeddings:\n",
        "\n",
        "```yaml\n",
        "features:\n",
        "  - name: word\n",
        "    vectorizer:\n",
        "      type: token1d\n",
        "    embeddings:\n",
        "      label: glove-840B\n",
        "  - name: word2\n",
        "    vectorizer:\n",
        "      type: token1d\n",
        "    embeddings:\n",
        "      label: w2v-gn\n",
        "```\n",
        "Above, we told the trainer to use 2 sets of pre-trained word embeddings, a large GloVe model as well as the word2vec GoogleNews vectors we used in the last example, and concatenate them together as 2 different features with 2 `token1d` vectorizers.  A more common case would be to provide different vectorizers or transformations on the surface data (for instance maybe providing mixed-case to one feature, and lower-case to another).  Notice in this example that we have actually eliminated the `transform_fn` parameter from our previous run, and that we are using the same `vectorizer` for each feature.\n",
        "\n",
        "In this case, because the `vectorizer`s are actually the same, `MEAD` also allows us to \"stack\" them as a single feature as follows:\n",
        "\n",
        "```yaml\n",
        "features:\n",
        "  - name: word\n",
        "    vectorizer:\n",
        "      type: token1d\n",
        "    embeddings:\n",
        "      label: [glove-840B, w2v-gn]\n",
        "\n",
        "```\n",
        "Finally, lets update the optimizer block and use [AdamW](https://www.fast.ai/2018/07/02/adam-weight-decay/#adamw) with a slightly different learning rate, and some weight decay:\n",
        "```yaml\n",
        "train:\n",
        "  epochs: 2\n",
        "  optim: adamw\n",
        "  eta: 0.0008\n",
        "  weight_decay: 1.0e-5\n",
        "  early_stopping_metric: acc\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dySwR7QFnSX9",
        "colab_type": "code",
        "outputId": "f16df1e4-53d9-44e3-ac02-52f37edf074f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!mead-train --config https://raw.githubusercontent.com/dpressel/mead-baseline/master/mead/config/sst2-lstm-pyt.yml"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading config file '/tmp/tmpaz_5pt1t'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/logging.json'\n",
            "No file found '/usr/local/l...', loading as string\n",
            "\u001b[33;1mWarning: no mead-settings file was found at [/usr/local/lib/python3.6/dist-packages/mead/config/mead-settings.json]\u001b[0m\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/datasets.json'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/embeddings.json'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/vecs.json'\n",
            "Task: [classify]\n",
            "using /root/.bl-data as data/embeddings cache\n",
            "Clean\n",
            "files for https://www.dropbox.com/s/7jyi4pi894bh2qh/sst2.tar.gz?dl=1 found in cache, not downloading\n",
            "[train file]: /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917/stsa.binary.phrases.train\n",
            "[valid file]: /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917/stsa.binary.dev\n",
            "[test file]: /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917/stsa.binary.test\n",
            "Progress: [========================================] 100%\n",
            "tcmalloc: large alloc 2176770048 bytes == 0x3a0a000 @  0x7f5001bd91e7 0x59203c 0x4ca610 0x56697a 0x5a4be1 0x5a5cda 0x4ce182 0x50a2bf 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50cd96 0x509758 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091\n",
            "extracting file..\n",
            "downloaded data saved in /root/.bl-data/8084fbacc2dee3b1fd1ca4cc534cbfff3519ed0d\n",
            "files for https://www.dropbox.com/s/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz?dl=1 found in cache\n",
            "embedding file location: /root/.bl-data/281bc75825fa6474e95a1de715f49a3b4e153822\n",
            "model file [./sst2-lstm-pyt/classify-model-1366.pyt]\n",
            "Doing early stopping on [acc] with patience [2]\n",
            "reporting [<bound method EpochReportingHook.step of <baseline.reporting.LoggingReporting object at 0x7f4ff9e34cf8>>]\n",
            "Calling model <function register_model.<locals>.create at 0x7f4fab782b70>\n",
            "LSTMModel(\n",
            "  (embeddings): EmbeddingsStack(\n",
            "    (embeddings): ModuleList(\n",
            "      (0): LookupTableEmbeddingsModel(\n",
            "        (embeddings): Embedding(17241, 300, padding_idx=0)\n",
            "      )\n",
            "      (1): LookupTableEmbeddingsModel(\n",
            "        (embeddings): Embedding(17241, 300, padding_idx=0)\n",
            "      )\n",
            "    )\n",
            "    (reduction): ConcatReduction()\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (pool_model): LSTMEncoderHidden(\n",
            "    (rnn): LSTM(600, 100, batch_first=True)\n",
            "  )\n",
            "  (stack_model): PassThru()\n",
            "  (output_layer): Dense(\n",
            "    (layer): Linear(in_features=100, out_features=2, bias=True)\n",
            "    (activation): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "adamw(eta=0.000800, beta1=0.900000, beta2=0.999000, epsilon=0.000000, wd=0.000010)\n",
            "Progress: [                                        ]   0%/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 1, \"phase\": \"Train\", \"acc\": 0.890619924377282, \"precision\": 0.9045740382086411, \"recall\": 0.8952412503845335, \"f1\": 0.8998834471111534, \"mcc\": 0.7794088393422552, \"avg_loss\": 0.26784765481944206}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 1, \"phase\": \"Valid\", \"acc\": 0.8497706422018348, \"precision\": 0.854875283446712, \"recall\": 0.8490990990990991, \"f1\": 0.8519774011299435, \"mcc\": 0.6994946143532945, \"avg_loss\": 0.3671970439886828}\n",
            "New best 0.850\n",
            "saving ./sst2-lstm-pyt/classify-model-1366.pyt\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 2, \"phase\": \"Train\", \"acc\": 0.9506893101700862, \"precision\": 0.9575779205329527, \"recall\": 0.952388840247048, \"f1\": 0.9549763314311476, \"mcc\": 0.9004934376490908, \"avg_loss\": 0.130510908326032}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 2, \"phase\": \"Valid\", \"acc\": 0.8555045871559633, \"precision\": 0.8767772511848341, \"recall\": 0.8333333333333334, \"f1\": 0.8545034642032333, \"mcc\": 0.7120853635082497, \"avg_loss\": 0.4270544017263509}\n",
            "New best 0.856\n",
            "saving ./sst2-lstm-pyt/classify-model-1366.pyt\n",
            "Best performance on acc: 0.856 at epoch 1\n",
            "Reloading best checkpoint\n",
            "adamw(eta=0.000800, beta1=0.900000, beta2=0.999000, epsilon=0.000000, wd=0.000010)\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 0, \"phase\": \"Test\", \"acc\": 0.8671059857221307, \"precision\": 0.8918918918918919, \"recall\": 0.834983498349835, \"f1\": 0.8625, \"mcc\": 0.7356778288968386, \"avg_loss\": 0.34617003810759234}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPaWS88johIZ",
        "colab_type": "text"
      },
      "source": [
        "# Transformers are the new craze in NLP!\n",
        "\n",
        "I covered them in my [Deep Learning Summer School Talk in Gdansk, 2019](https://docs.google.com/presentation/d/1DJI1yX4U5IgApGwavt0AmOCLWwso7ou1Un93sMuAWmA/edit#slide=id.p) including [how they are implemented and how to fine-tune BERT from scratch in this colab](https://github.com/dpressel/dliss-tutorial/blob/master/3_finetuning.ipynb).  In fact, the source code in the tutorial is based on the source code inside of the [mead-layers 8-mile API](https://github.com/dpressel/mead-baseline/tree/master/layers/eight_mile) within `mead-baseline`.\n",
        "\n",
        "As you probably expect then, it should be very easy to use `mead-baseline` to fine-tune [BERT](https://arxiv.org/abs/1810.04805), the now ubiquitous bidirectional Transformer encoder (and you would be correct!).\n",
        "\n",
        "The 8-mile API provides a full implementation of Transformers in both TensorFlow and PyTorch, and we provide deep-learning platform-independent checkpoints in the `numpy NPZ` format so you can easily use BERT from either framework.  This also means that you can run the exact same configuration no matter which backend you are using, TensorFlow or PyTorch by just changing the `backend` either through the command-line or in the config file!\n",
        "\n",
        "The configuration looks a bit different, we will cover each difference in detail:\n",
        "\n",
        "```yaml\n",
        "model:\n",
        "  model_type: fine-tune\n",
        "\n",
        "```\n",
        "The model block is quite simple here -- we just tell the driver we want to run a type of \"fine-tune\" model.  We will get into the details of how the code is organized later, but for now, think of a \"fine-tune\" model as the pre-training language model graph minus the final output to the vocabulary, grafted onto a single linear layer that projects to the logit space.  In `mead-baseline`, for fine-tuning on downstream tasks, the entire BERT model is considered as an embedding feature followed by the final projection layer.\n",
        "\n",
        "```yaml\n",
        "train:\n",
        "  early_stopping_metric: acc\n",
        "  epochs: 5\n",
        "  eta: 4.0e-5\n",
        "  optim: adamw\n",
        "  weight_decay: 1.0e-3\n",
        "```\n",
        "The training routine has a slightly different learning rate and weight decay from our previous LSTM example, but otherwise, its basically the same.\n",
        "\n",
        "The next section gets a bit more complicated, but it will make sense when we explain the details of BERT.  \n",
        "\n",
        "```yaml\n",
        "features:\n",
        "- embeddings:\n",
        "    word_embed_type: learned-positional-w-bias\n",
        "    label: bert-base-uncased-npz\n",
        "    type: tlm-words-embed-pooled\n",
        "    reduction: sum-layer-norm\n",
        "    layer_norms_after: true\n",
        "    finetune: true\n",
        "    dropout: 0.1\n",
        "    mlm: true\n",
        "  name: bert\n",
        "  vectorizer:\n",
        "    label: bert-base-uncased\n",
        "\n",
        "```\n",
        "First the checkpoint we are referencing is defined in the `label`.  Just as with our word embeddings, if you check the embeddings index, you will find an entry for this:\n",
        "\n",
        "```\n",
        "  {\n",
        "    \"label\": \"bert-base-uncased-npz\",\n",
        "    \"file\": \"https://www.dropbox.com/s/3ivk6npc6e0bgyk/bert-base-uncased.npz?dl=1\",\n",
        "    \"sha1\": \"54bef0c84ce29a7729c5e1fc3509a01f8c579891\",\n",
        "    \"unzip\": false,\n",
        "    \"dsz\": 768\n",
        "  },\n",
        "\n",
        "```\n",
        "\n",
        "This is nice -- `mead-train` doesnt really know, nor care, what type of embeddings you have or how they are orchestrated.  It just knows that embeddings are defined in the `features` block, under the `embeddings` section, and that they may be downloaded from the internet to our cache if they are listed as a URL.\n",
        "\n",
        "Next, the `vectorizer` entry also uses a `label` to reference a vectorizer.  This is defined in the `vectorizer` index which can be passed into `mead-train` or defaulted to the installed [vecs.json](https://github.com/dpressel/mead-baseline/blob/master/mead/config/vecs.json).  Here is the entry for the `bert-base-uncased` vectorizer:\n",
        "\n",
        "```\n",
        "  {\n",
        "    \"label\": \"bert-base-uncased\",\n",
        "    \"type\": \"wordpiece1d\",\n",
        "    \"vocab_file\": \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n",
        "    \"transform\": \"baseline.lowercase\"\n",
        "  },\n",
        "```\n",
        "You might be wondering why its referencing the HuggingFace repository if there are no dependencies from `mead-baseline`.  The good people at HuggingFace have made the official BERT vocabularies available for download, primarily for their excellent `transformers` library to be able to use, so there is no need to upload it elsewhere.  And although we are not using HuggingFace `transformers` in this example, we do provide `addon` integration that makes it possible to use their implementation if preferred.\n",
        "\n",
        "## A Deep Dive into the BERT Fine-Tuning Config\n",
        "\n",
        "BERT is just another Transformer, and so we can use MEAD's `Transformer` layers to implement BERT.  However, if it was just the original `Transformer` it wouldnt have such a cool name, right?\n",
        "\n",
        "### What Kind of Transformer is BERT Exactly?\n",
        "\n",
        "BERT is based very closely on the original Transformer paper [Attention is all you Need, Vaswani et al 2017](https://arxiv.org/abs/1706.03762) and the source repository [Tensor2Tensor, AKA T2T](https://github.com/tensorflow/tensor2tensor), mostly via [GPT, a Transformer language model (LM) with a causal objective](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
        "\n",
        "After the original Transformer paper came out, a bunch of researchers studied the location of the layer normalization in each Transformer layer, and realized it was [sub-optimally located](https://arxiv.org/abs/2002.04745).  Since then, most implementations have moved the layer norm to the beginning of the block, but BERT doesnt -- it follows T2T!\n",
        "\n",
        "See this tweet from Colin Raffel, first author on the [T5 paper](https://arxiv.org/abs/1910.10683), and the follow-on thread for a recent reference to the layer norm location: https://twitter.com/colinraffel/status/1250853464864784384\n",
        "\n",
        "There are a few difference in BERT's architecture from T2T:\n",
        "\n",
        "1. it has no Decoder!  It basically follows the GPT implementation, treating the Encoder as an LM\n",
        "1. it uses learned positional embeddings instead of the sinusoidal positional embeddings (again following GPT)\n",
        "1. It has an additional feature known as token-type embeddings which are typically intended to delineate a demarcation along sentence boundaries.\n",
        "\n",
        "BERT additionally differs from GPT in that it does not use a causal LM objective.  Instead it is trained on a Masked Language Model objective and a Next Sentence Prediction (NSP) objective.  More recent flavors of BERT like [XLM](https://arxiv.org/abs/1901.07291) and [RoBERTa](https://arxiv.org/abs/1907.11692) have abandoned the NSP objective, finding that it actually hurts downstream performance on the common benchmarks.\n",
        "\n",
        "Armed with all this info, lets take another glance at the `embeddings` sub-block where we define our BERT representation:\n",
        "\n",
        "```\n",
        "- embeddings:\n",
        "    word_embed_type: learned-positional-w-bias\n",
        "    label: bert-base-uncased-npz\n",
        "    type: tlm-words-embed-pooled\n",
        "    reduction: sum-layer-norm\n",
        "    layer_norms_after: true\n",
        "    finetune: true\n",
        "    dropout: 0.1\n",
        "    mlm: true\n",
        "```\n",
        "\n",
        "The `type` field identifies a Transformer LM embedding that uses a pooled representation of words -- well sub-words really, via [WordPiece](https://github.com/google/sentencepiece) or [Byte-Pair Encoding](https://www.aclweb.org/anthology/P16-1162/) and we are `finetune`-ing an `mlm` model.  As discussed above, we want to place the `layer_norms_after` the block, not before it.\n",
        "\n",
        "Because it does layer norms after the attention and at the end of each block, it makes sense that BERT places a layer norm right at the end of the embeddings.  In MEAD, a single object manages all of the embedddings and its called an [EmbeddingsStack](https://github.com/dpressel/mead-baseline/blob/master/layers/eight_mile/pytorch/layers.py).  It has a `reduction` operator that tells it how to combine multiple features.  The API also has an object called a `LearnedPositionalLookupTableEmbeddings` which combines positional embeddings with regular word embeddings (by summing them together).  If we were using the previously described token-type embeddings, we would construct another feature maybe named `token-type` and assign a vectorizer that would create 0s for the first sentence and 1s for the second sentence.  We could then combine these using the `sum-layer-norm` `reduction`, which would reduce to:\n",
        "\n",
        "```\n",
        "LayerNorm(PositionEmbed(IndexOf(x)) + WordEmbed(x)) + TokenTypeEmbed(SentIndexOf(x)))\n",
        "```\n",
        "Thats exactly the same operations BERT does, MEAD just provides a simple composition making it easier to define models without code.  A twist: if the TokenType is almost always ignored, we could provide a 0-tensor and still use a LookupTable embedding underneath.\n",
        "\n",
        "In PyTorch this would be implemented as an `nn.Embedding(2, 768)` and we would always pass `zeros_like(x)`, which would cause a memory allocation every time we have different input.  This is inefficient and a bit complicated for simple fine-tuning.\n",
        "\n",
        "If we think about it a bit different, in the case where we dont care about the token type ID we would really just be using the `embedding.weight[0]` and adding it to all input.  Well, thats just a bias, and if we had a `LearnedPositionalLookupTableEmbeddingsWithBias` object that allowed this, we could just use that... and of course, we do, which is the embedding type referred to in the block above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ16YP1r09o_",
        "colab_type": "text"
      },
      "source": [
        "For consistency and readability (YAML is easier to read IMO), up to this point, I have been showing the details of fine-tuning BERT for SST2 using this config:\n",
        "\n",
        "https://github.com/dpressel/mead-baseline/blob/master/mead/config/sst2-bert-base-uncased.yml\n",
        "\n",
        "And, if you have Colab Pro or are running this on a machine with some GPU horsepower, you can run the example above like this:\n",
        "\n",
        "```\n",
        "!mead-train --config https://raw.githubusercontent.com/dpressel/mead-baseline/master/mead/config/sst2-bert-base-uncased.yml\n",
        "```\n",
        "Because by default Colab is pretty slow, my example below is taken from the TREC dataset, not SST2.  On the plus side, this is the exact same dataset that I fine-tuned in my DLSS Tutorial, and it runs in a couple of minutes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OWQUgUVp9_r",
        "colab_type": "code",
        "outputId": "0f3bb1f1-6ad6-4997-a62c-625042df28b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!mead-train --config https://raw.githubusercontent.com/dpressel/mead-baseline/master/mead/config/trec-bert-base-uncased.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading config file '/tmp/tmp69__0rgw'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/logging.json'\n",
            "No file found '/usr/local/l...', loading as string\n",
            "\u001b[33;1mWarning: no mead-settings file was found at [/usr/local/lib/python3.6/dist-packages/mead/config/mead-settings.json]\u001b[0m\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/datasets.json'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/embeddings.json'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/vecs.json'\n",
            "Task: [classify]\n",
            "using /root/.bl-data as data/embeddings cache\n",
            "Downloading https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
            "Progress: [========================================] 100%\n",
            "extracting file..\n",
            "downloaded data saved in /root/.bl-data/b52dd4b6cfe6ec51ea6cfa56baef8128b1785e3b\n",
            "[train file]: /root/.bl-data/b52dd4b6cfe6ec51ea6cfa56baef8128b1785e3b/trec.nodev.utf8\n",
            "[valid file]: /root/.bl-data/b52dd4b6cfe6ec51ea6cfa56baef8128b1785e3b/trec.dev.utf8\n",
            "[test file]: /root/.bl-data/b52dd4b6cfe6ec51ea6cfa56baef8128b1785e3b/trec.test.utf8\n",
            "files for https://www.dropbox.com/s/3ivk6npc6e0bgyk/bert-base-uncased.npz?dl=1 found in cache\n",
            "embedding file location: /root/.bl-data/54bef0c84ce29a7729c5e1fc3509a01f8c579891\n",
            "model file [./trec-bert/classify-model-2381.pyt]\n",
            "Doing early stopping on [acc] with patience [5]\n",
            "reporting [<bound method EpochReportingHook.step of <baseline.reporting.LoggingReporting object at 0x7f07b86b7cc0>>]\n",
            "Calling model <function register_model.<locals>.create at 0x7f076a004f28>\n",
            "FineTuneModelClassifier(\n",
            "  (embeddings): EmbeddingsStack(\n",
            "    (embeddings): ModuleList(\n",
            "      (0): TransformerLMPooledEmbeddingsModel(\n",
            "        (embeddings): EmbeddingsStack(\n",
            "          (embeddings): ModuleList(\n",
            "            (0): LearnedPositionalLookupTableEmbeddingsWithBiasModel(\n",
            "              (embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (pos_embeddings): Embedding(512, 768)\n",
            "            )\n",
            "          )\n",
            "          (reduction): SumLayerNormReduction(\n",
            "            (ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (transformer): TransformerEncoderStack(\n",
            "          (encoders): ModuleList(\n",
            "            (0): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (2): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (3): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (4): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (5): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (6): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (7): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (8): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (9): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (10): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (11): TransformerEncoder(\n",
            "              (self_attn): MultiHeadedAttention(\n",
            "                (w_Q): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_K): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_V): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (w_O): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (attn_fn): SeqScaledDotProductAttention(\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (ffn): Sequential(\n",
            "                (0): Dense(\n",
            "                  (layer): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "                (1): GeLU()\n",
            "                (2): Dropout(p=0.0, inplace=False)\n",
            "                (3): Dense(\n",
            "                  (layer): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (activation): Identity()\n",
            "                )\n",
            "              )\n",
            "              (ln1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (ln2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (ln): Identity()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (reduction): ConcatReduction()\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (stack_model): PassThru()\n",
            "  (output_layer): Dense(\n",
            "    (layer): Linear(in_features=768, out_features=6, bias=True)\n",
            "    (activation): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "adamw(eta=0.000010, beta1=0.900000, beta2=0.999000, epsilon=0.000000, wd=0.000000)\n",
            "Progress: [                                        ]   0%/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 1, \"phase\": \"Train\", \"acc\": 0.8204, \"mean_precision\": 0.7828487872942658, \"mean_recall\": 0.7549658312684628, \"macro_f1\": 0.7667246682699105, \"weighted_precision\": 0.8223518570966335, \"weighted_recall\": 0.8204, \"weighted_f1\": 0.8207415568977303, \"r_k\": 0.7753623699906528, \"avg_loss\": 0.5063090516887605}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 1, \"phase\": \"Valid\", \"acc\": 0.9358407079646017, \"mean_precision\": 0.8934356949641492, \"mean_recall\": 0.9048274448119168, \"macro_f1\": 0.8984372281940712, \"weighted_precision\": 0.9366141723781045, \"weighted_recall\": 0.9358407079646017, \"weighted_f1\": 0.9359541643230652, \"r_k\": 0.9200723079423354, \"avg_loss\": 0.2145082916424659}\n",
            "New best 0.936\n",
            "saving ./trec-bert/classify-model-2381.pyt\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 2, \"phase\": \"Train\", \"acc\": 0.9596, \"mean_precision\": 0.9577278265588199, \"mean_recall\": 0.9232428415598871, \"macro_f1\": 0.9379250364132196, \"weighted_precision\": 0.9595053278396865, \"weighted_recall\": 0.9596, \"weighted_f1\": 0.9593059406651188, \"r_k\": 0.9495145806156452, \"avg_loss\": 0.13992060064780526}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 2, \"phase\": \"Valid\", \"acc\": 0.9424778761061947, \"mean_precision\": 0.8896661653683844, \"mean_recall\": 0.8884137390866168, \"macro_f1\": 0.8888257366347773, \"weighted_precision\": 0.9424544059887662, \"weighted_recall\": 0.9424778761061947, \"weighted_f1\": 0.9421982782014809, \"r_k\": 0.9283502970341686, \"avg_loss\": 0.25527186908555577}\n",
            "New best 0.942\n",
            "saving ./trec-bert/classify-model-2381.pyt\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 3, \"phase\": \"Train\", \"acc\": 0.9784, \"mean_precision\": 0.9758646008023604, \"mean_recall\": 0.9533517402954326, \"macro_f1\": 0.9636075108502649, \"weighted_precision\": 0.9783694406371587, \"weighted_recall\": 0.9784, \"weighted_f1\": 0.9782819204194013, \"r_k\": 0.9730181720149151, \"avg_loss\": 0.07352684313745704}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 3, \"phase\": \"Valid\", \"acc\": 0.9358407079646017, \"mean_precision\": 0.8919728702305104, \"mean_recall\": 0.9058760537324391, \"macro_f1\": 0.8983760828422734, \"weighted_precision\": 0.9361114093727713, \"weighted_recall\": 0.9358407079646017, \"weighted_f1\": 0.9358674678395297, \"r_k\": 0.9200465931170431, \"avg_loss\": 0.25808379781414165}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 4, \"phase\": \"Train\", \"acc\": 0.9896, \"mean_precision\": 0.989749921530145, \"mean_recall\": 0.9779988729797328, \"macro_f1\": 0.98362439886463, \"weighted_precision\": 0.9895968711540103, \"weighted_recall\": 0.9896, \"weighted_f1\": 0.9895704038317328, \"r_k\": 0.9870118086525355, \"avg_loss\": 0.03346560426609358}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 4, \"phase\": \"Valid\", \"acc\": 0.9491150442477876, \"mean_precision\": 0.8986230454719996, \"mean_recall\": 0.9383633485600358, \"macro_f1\": 0.9145071299638236, \"weighted_precision\": 0.9514826858525492, \"weighted_recall\": 0.9491150442477876, \"weighted_f1\": 0.9496623755852825, \"r_k\": 0.9368474068284844, \"avg_loss\": 0.30114864344191483}\n",
            "New best 0.949\n",
            "saving ./trec-bert/classify-model-2381.pyt\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 5, \"phase\": \"Train\", \"acc\": 0.994, \"mean_precision\": 0.9953474036869717, \"mean_recall\": 0.9872675955597372, \"macro_f1\": 0.9911957228374785, \"weighted_precision\": 0.9940142671215609, \"weighted_recall\": 0.994, \"weighted_f1\": 0.9939940422680987, \"r_k\": 0.9925073204444955, \"avg_loss\": 0.021193926515094063}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 5, \"phase\": \"Valid\", \"acc\": 0.9402654867256637, \"mean_precision\": 0.8852631416987853, \"mean_recall\": 0.8885301302485566, \"macro_f1\": 0.886646082326069, \"weighted_precision\": 0.9398122519146789, \"weighted_recall\": 0.9402654867256637, \"weighted_f1\": 0.9397049691063271, \"r_k\": 0.9257296002676005, \"avg_loss\": 0.35310900185250643}\n",
            "Best performance on acc: 0.949 at epoch 3\n",
            "Reloading best checkpoint\n",
            "adamw(eta=0.000010, beta1=0.900000, beta2=0.999000, epsilon=0.000000, wd=0.000000)\n",
            "Progress: [========================================] 100%\n",
            "                 3        1        5        4        0        2\n",
            "        3       64        1        0        0        0        0\n",
            "        1        2       85        1        2        4        0\n",
            "        5        0        0      113        0        0        0\n",
            "        4        0        0        0       79        2        0\n",
            "        0        1        0        0        0      137        0\n",
            "        2        0        0        0        0        1        8\n",
            "\n",
            "\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 0, \"phase\": \"Test\", \"acc\": 0.972, \"mean_precision\": 0.9769202624433179, \"mean_recall\": 0.9576369763028207, \"macro_f1\": 0.9663087410058507, \"weighted_precision\": 0.9725939351589663, \"weighted_recall\": 0.972, \"weighted_f1\": 0.971731956139051, \"r_k\": 0.9649371893288525, \"avg_loss\": 0.1767210774421692}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87KSW7DS2HRy",
        "colab_type": "text"
      },
      "source": [
        "The results above are quite good, in fact, that particular run did quite a bit better than our final results in the [DLSS Tutorial](https://github.com/dpressel/dliss-tutorial/blob/master/3_finetuning.ipynb).  Now we have seen examples of using `MEAD`'s built-in Baselines to do really well on a couple of datasets.\n",
        "\n",
        "# But How Does It Work Under the Hood?\n",
        "\n",
        "You might be wondering \"how does it work, and how can I use this for my own research or model development\"?  So far, our introduction has been quite high level -- all we have done is download a python package that installed a magic command `mead-train` that can run some configuration files.\n",
        "\n",
        "The `mead-baseline` software can do quite a bit of things to streamline your research. It provides strong baselines on publicly available, automatically downloadable datasets out of the box, and provides software to help you track and persist your results and models.\n",
        "\n",
        "It also allows you, via the `datasets`, `embeddings` and `vecs` optional indices to easily define your own components that can improve on the existing models or run new models.  The `mead-train` driver follows the pattern of [Inversion of Control](https://en.wikipedia.org/wiki/Inversion_of_control) and allows you, the user, to define your own components for the framework including your own custom:\n",
        "\n",
        "- readers\n",
        "- vectorizers\n",
        "- embeddings\n",
        "- models\n",
        "- reporting and logging\n",
        "- trainer\n",
        "- fit function\n",
        "- task\n",
        "\n",
        "The details are out-of-scope of this tutorial, but if you want to know more, there is [documentation](https://github.com/dpressel/mead-baseline/blob/master/docs/addons.md) on how `addons` are supported in the software.\n",
        "\n",
        "In this tutorial, we will focus on a single `addon` type for the purposes of classification -- we will be creating our own classification models, and in doing so, we can explain the idioms that are built into `MEAD`\n",
        "\n",
        "## Tasks in MEAD\n",
        "\n",
        "Under the hood, `mead-train` delegates all of its work to somebody else.  It basically does nothing except knowing who to call when, and that information is just the information that it receives from its configuration file.  If the `task` is `classify`, it knows to proxy the information to a `mead.ClassifyTask` which is a type of `mead.Task` that is registered to listen for those `classify` requests and train them.  The base `mead.Task` can be thought of as a recipe for creating a classifier.  It knows how to load `vectorizers`, `readers` and `trainer`s, and how to register the backend deep learning framework, but it still knows nothing about the actual classifier it is running -- it actually just calls the `baseline.train.fit()` method, which is defined like this:\n",
        "\n",
        "```python\n",
        "@export\n",
        "def fit(model_params, ts, vs, es, **kwargs):\n",
        "    \"\"\"This method delegates to the registered fit function for each DL framework.  It is possible to provide a by-pass\n",
        "    to our defined fit functions for each method (this is considered advanced usage).  In cases where the user wishes\n",
        "    to provide their own fit hook, the need to decorate the bypass hook with @register_training_func(name='myname'),\n",
        "    and then pass in the `fit_func='myname'` to this.  MEAD handles this automatically -- just pass fit_func: myname\n",
        "    in the mead config if you want your own bypass, in which case training is entirely delegate to the 3rd party code.\n",
        "\n",
        "    This use-case is expected to be extremely uncommon.  More common behavior would be to override the Trainer and use\n",
        "    the provided fit function.\n",
        "\n",
        "    :param model:\n",
        "    :param ts:\n",
        "    :param vs:\n",
        "    :param es:\n",
        "    :param kwargs:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if type(model_params) is dict:\n",
        "        task_name = model_params['task']\n",
        "    else:\n",
        "        task_name = model_params.task_name\n",
        "    fit_func_name = kwargs.get('fit_func', 'default')\n",
        "    return BASELINE_FIT_FUNC[task_name][fit_func_name](model_params, ts, vs, es, **kwargs)\n",
        "\n",
        "```\n",
        "Each task has a defined `default` fit function -- with at least one implementation per framework.  In fact, for the TensorFlow backend, we define 4 handlers including 2 defaults, one for eager mode and one for declarative mode:\n",
        "\n",
        "- declarative mode `default` (using tf.datasets)\n",
        "- eager mode `default` (using tf eager execution)\n",
        "- `distributed` mode (using tf eager execution)\n",
        "- `feed_dict` declarative mode using placeholders\n",
        "\n",
        "The `fit_func` normally should be left alone, but in some cases, you may want to define it to one of the other approaches, or you might even want to make your own (though this is fairly uncommon).\n",
        "\n",
        "### Framework Implementations\n",
        "\n",
        "So far, notice that every software package we have used is either under `mead` or `baseline`.  The actual code that implements a backend like PyTorch, lives in a sub-module following the naming:\n",
        "\n",
        "```\n",
        "baseline.{framework}.{task}\n",
        "```\n",
        "\n",
        "In our case, for this tutorial, we are interested in the details of the PyTorch classification package, which is: [baseline.pytorch.classify](https://github.com/dpressel/mead-baseline/tree/master/baseline/pytorch/classify).  Thankfully, the PyTorch implementation is quite simple, containing only 3 files:\n",
        "\n",
        "```\n",
        "(mead) dpressel@dpressel-CORSAIR-ONE:~/dev/work/baseline/baseline/pytorch/classify$ ls -l\n",
        "total 32\n",
        "-rw-r--r-- 1 dpressel dpressel    92 Apr 29 21:19 __init__.py\n",
        "-rw-r--r-- 1 dpressel dpressel 15831 May  6 16:17 model.py\n",
        "-rw-r--r-- 1 dpressel dpressel  8499 May  6 16:17 train.py\n",
        "\n",
        "```\n",
        "\n",
        "The [model.py](https://github.com/dpressel/mead-baseline/tree/master/baseline/pytorch/classify/model.py) contains the PyTorch abstract base classes that extend `baseline.ClassifierModel`: \n",
        "\n",
        "```\n",
        "class ClassifierModelBase(nn.Module, ClassifierModel)\n",
        "class EmbedPoolStackClassifier(ClassifierModelBase)\n",
        "```\n",
        "\n",
        "These are also implemented in TensorFlow under [baseline.tf.classify.model](https://github.com/dpressel/mead-baseline/tree/master/baseline/tf/classify/model.py)\n",
        "\n",
        "Additionally, there are several concrete classes that inherit either `EmbedPoolStackClassifier` or `ClassifierModelBase`.  Each of these models uses `@register_model()` to declare themselves to the model registry.\n",
        "\n",
        "#### A Deep-Dive Into the LSTM and Convolutional Models\n",
        "\n",
        "Both the convolutional and LSTM classifiers that we used inherit the `EmbedPoolStackClassifier`.  It has a funny but pretty precise name, named after the following stages\n",
        "\n",
        "- *embed* the input, one or more temporal tensors, typically of shape `[B, T]`, where `B` is the batch size (AKA `batchsz`) and `T` is the temporal length (e.g. the number of words in a sentence) with an `EmbeddingsStack` followed by a `reduction` if more than one to a tensor of shape `[B, T, H]` where `H` is the number of hidden units\n",
        "- *pool* the `[B, T, H]` tensor to a fixed length representation, with possible a different number of hidden units (lets call it `P`) of shape `[B, P]`\n",
        "- Use a *stack* of hidden layers to transform the input `[B, P]` into some output `[B, S]`.  This is typically one or more `Dense` layers with activations in between (possibly also with residual connections, dropout and layer norms)\n",
        "- *output* project the input `[B, S]` to a tensor of shape `[B, L]` where `L` is the number of output layers.\n",
        "\n",
        "Not surprisingly, the `EmbedPoolStackClassifier` provides hooks for each step, but most sub-classes only need to implement the `init_pool` method:\n",
        "\n",
        "```python\n",
        "class EmbedPoolStackClassifier(ClassifierModelBase):\n",
        "\n",
        "    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer\n",
        "    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer\n",
        "    def init_stacked(self, input_dim: int, **kwargs) -> BaseLayer\n",
        "    def init_output(self, input_dim: int, **kwargs) -> BaseLayer\n",
        "```\n",
        "\n",
        "In the case of our convolutional model, we simply inherit the base functionality from this idiomatic base-class, and provide a pooling operation using parallel convolutions, followed by a max-over-time pooling.\n",
        "\n",
        "For the LSTM, its the same thing, but in this case our pooling layer uses the final LSTM hidden state.  In fact, here is the entire RNN model:\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "@register_model(task='classify', name='lstm')\n",
        "class LSTMModel(EmbedPoolStackClassifier):\n",
        "    \"\"\"A simple single-directional single-layer LSTM. No layer-stacking.\n",
        "    \"\"\"\n",
        "\n",
        "    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n",
        "        \"\"\"LSTM with dropout yielding a final-state as output\n",
        "\n",
        "        :param input_dim: The input word embedding depth\n",
        "        :param kwargs: See below\n",
        "\n",
        "        :Keyword Arguments:\n",
        "        * *rnnsz* -- (``int``) The number of hidden units (defaults to `hsz`)\n",
        "        * *rnntype/rnn_type* -- (``str``) The RNN type, defaults to `lstm`, other valid values: `blstm`\n",
        "        * *hsz* -- (``int``) backoff for `rnnsz`, typically a result of stacking params.  This keeps things simple so\n",
        "          its easy to do things like residual connections between LSTM and post-LSTM stacking layers\n",
        "\n",
        "        :return: A pooling layer\n",
        "        \"\"\"\n",
        "        unif = kwargs.get('unif')\n",
        "        hsz = kwargs.get('rnnsz', kwargs.get('hsz', 100))\n",
        "        if type(hsz) is list:\n",
        "            hsz = hsz[0]\n",
        "        weight_init = kwargs.get('weight_init', 'uniform')\n",
        "        rnntype = kwargs.get('rnn_type', kwargs.get('rnntype', 'lstm'))\n",
        "        if rnntype == 'blstm':\n",
        "            return BiLSTMEncoderHidden(input_dim, hsz, 1, self.pdrop, unif=unif, batch_first=True, initializer=weight_init)\n",
        "        return LSTMEncoderHidden(input_dim, hsz, 1, self.pdrop, unif=unif, batch_first=True, initializer=weight_init)\n",
        "```\n",
        "\n",
        "The layers returned by this model are defined in the `mead-layers` package which provides TensorFlow and PyTorch NLP-specific utilities, but you could easily define your own layers.\n",
        "\n",
        "Note that we use `register_model` to identify what `task` this class is serving as well as a unique identifier that we give in the mead configuration file's `model` block under `type`.\n",
        "\n",
        "#### A Deep-Dive Into the Fine-Tuning Model Implementation\n",
        "\n",
        "The Fine-Tuning Model is pretty robust, but most often, its just the simplest thing possibe -- a single output layer.  However, the same hooks are provided as in the `EmbedPoolStackClassifier` minus the pooling, as we assume that the embedding is Bring Your Own Pooler (BYOP).  Unlike the previous example, we typically dont need to override this class, it gives us everything we need.\n",
        "\n",
        "```python\n",
        "@register_model(task='classify', name='fine-tune')\n",
        "class FineTuneModelClassifier(ClassifierModelBase):\n",
        "    \"\"\"Fine-tune based on pre-pooled representations\"\"\"\n",
        "\n",
        "    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer\n",
        "    def init_stacked(self, input_dim: int, **kwargs) -> BaseLayer\n",
        "\n",
        "    def init_output(self, input_dim: int, **kwargs) -> BaseLayer\n",
        "\n",
        "```\n",
        "\n",
        "#### Defining Your Own Models\n",
        "\n",
        "Just to demonstrate, lets make a pretty simple example.  We will create a layered conv. net, gradually broadening the receptive field.  Finally, we will take the max-over-time and the mean-over-time of the last layer and concatenate them.\n",
        "\n",
        "With PyTorch we could define this quite simply, but the `mead-layers` package, AKA `8 mile` makes this pretty trivial.  Our model is defined like this:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple\n",
        "from baseline.pytorch.classify import EmbedPoolStackClassifier\n",
        "from baseline.model import register_model\n",
        "from eight_mile.pytorch.layers import ConvEncoderStack, MeanPool1D, MaxPool1D\n",
        "\n",
        "class ResidualConvPool(nn.Module):\n",
        "    def __init__(self, input_dim: int, **kwargs):\n",
        "        super().__init__()\n",
        "        filtsz = kwargs['filtsz']\n",
        "        dropout = kwargs.get('dropout', 0.1)\n",
        "        hsz = kwargs.get('hsz', 300)\n",
        "        activation = kwargs.get('activation', 'relu')\n",
        "        nlayers = kwargs.get('layers', 3)\n",
        "        self.convs = ConvEncoderStack(input_dim, hsz, filtsz, nlayers=nlayers, pdrop=dropout, activation=activation)\n",
        "        self.output_dim = 2 * hsz\n",
        "        self.mean_pool = MeanPool1D(hsz)\n",
        "        self.max_pool = MaxPool1D(hsz)\n",
        "\n",
        "    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
        "        x, lengths = inputs\n",
        "        outputs = self.convs(x)\n",
        "        mx = self.max_pool(outputs)\n",
        "        mu = self.mean_pool((outputs, lengths))\n",
        "        output = torch.cat([mx, mu], -1)\n",
        "        return output\n",
        "\n",
        "@register_model(task='classify', name='resconv')\n",
        "class ConvClassifier(EmbedPoolStackClassifier):\n",
        "\n",
        "    def init_pool(self, input_dim: int, **kwargs) -> nn.Module:\n",
        "        return ResidualConvPool(input_dim, **kwargs)\n",
        "\n",
        "```\n",
        "\n",
        "Our config might look like very similar to our previous models with just a few differences:\n",
        "\n",
        "```yaml\n",
        "batchsz: 50\n",
        "basedir: sst2-pyt\n",
        "modules: [https://raw.githubusercontent.com/mead-ml/hub/master/v1/addons/resconv.py]\n",
        "```\n",
        "We can tell `mead-train` to go find our addon (it can be a file system full path, a URL, or anything in the python path).  Here we are referencing an online location.\n",
        "\n",
        "```yaml\n",
        "model:\n",
        "  model_type: resconv\n",
        "  filtsz: 3\n",
        "  dropout: 0.5\n",
        "\n",
        "```\n",
        "This section looks almost like the previous examples, but our model is named `resconv`, the same value we registered for our class name.\n",
        "\n",
        "This time, when we run, it we will use the `--x` arguments to override fields within the YAML.  In this case, we will pass in an override for the number of epochs to train, overriding the value in the file.  You can override any fields within the YAML file by defining them with `.`-delimited names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orxeV34Gfr9h",
        "colab_type": "code",
        "outputId": "f798c9ac-b808-4f18-d11d-ff29213e192d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!mead-train --config https://raw.githubusercontent.com/dpressel/mead-baseline/master/mead/config/sst2-resconv.yml --x:train.epochs 2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading config file '/tmp/tmpagu0d5by'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/logging.json'\n",
            "No file found '/usr/local/l...', loading as string\n",
            "\u001b[33;1mWarning: no mead-settings file was found at [/usr/local/lib/python3.6/dist-packages/mead/config/mead-settings.json]\u001b[0m\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/datasets.json'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/embeddings.json'\n",
            "Reading config file '/usr/local/lib/python3.6/dist-packages/mead/config/vecs.json'\n",
            "Task: [classify]\n",
            "using /root/.bl-data as data/embeddings cache\n",
            "Clean\n",
            "using /root/.bl-data as data/addons cache\n",
            "Progress: [========================================] 100%\n",
            "files for https://www.dropbox.com/s/7jyi4pi894bh2qh/sst2.tar.gz?dl=1 found in cache, not downloading\n",
            "[train file]: /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917/stsa.binary.phrases.train\n",
            "[valid file]: /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917/stsa.binary.dev\n",
            "[test file]: /root/.bl-data/31eb669609c65af3aa68a381fc760c4eaf801917/stsa.binary.test\n",
            "files for https://www.dropbox.com/s/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz?dl=1 found in cache\n",
            "embedding file location: /root/.bl-data/281bc75825fa6474e95a1de715f49a3b4e153822\n",
            "model file [sst2-pyt/classify-model-512.pyt]\n",
            "Doing early stopping on [acc] with patience [2]\n",
            "reporting [<bound method EpochReportingHook.step of <baseline.reporting.LoggingReporting object at 0x7fdb1d5b8128>>]\n",
            "Calling model <function register_model.<locals>.create at 0x7fdad25797b8>\n",
            "ConvClassifier(\n",
            "  (embeddings): EmbeddingsStack(\n",
            "    (embeddings): ModuleList(\n",
            "      (0): LookupTableEmbeddingsModel(\n",
            "        (embeddings): Embedding(17241, 300, padding_idx=0)\n",
            "      )\n",
            "    )\n",
            "    (reduction): ConcatReduction()\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (pool_model): ResidualConvPool(\n",
            "    (convs): ConvEncoderStack(\n",
            "      (layers): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): BTH2BHT()\n",
            "          (1): ConvEncoder(\n",
            "            (conv): Sequential(\n",
            "              (0): Conv1DSame(\n",
            "                (conv): Sequential(\n",
            "                  (0): ConstantPad1d(padding=(1, 1), value=0.0)\n",
            "                  (1): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
            "                )\n",
            "              )\n",
            "              (1): ReLU()\n",
            "              (2): Dropout(p=0.5, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): ResidualBlock(\n",
            "          (layer): ConvEncoder(\n",
            "            (conv): Sequential(\n",
            "              (0): Conv1DSame(\n",
            "                (conv): Sequential(\n",
            "                  (0): ConstantPad1d(padding=(1, 1), value=0.0)\n",
            "                  (1): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
            "                )\n",
            "              )\n",
            "              (1): ReLU()\n",
            "              (2): Dropout(p=0.5, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (2): ResidualBlock(\n",
            "          (layer): ConvEncoder(\n",
            "            (conv): Sequential(\n",
            "              (0): Conv1DSame(\n",
            "                (conv): Sequential(\n",
            "                  (0): ConstantPad1d(padding=(1, 1), value=0.0)\n",
            "                  (1): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
            "                )\n",
            "              )\n",
            "              (1): ReLU()\n",
            "              (2): Dropout(p=0.5, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (3): BHT2BTH()\n",
            "      )\n",
            "    )\n",
            "    (mean_pool): MeanPool1D(batch_first=True)\n",
            "    (max_pool): MaxPool1D(batch_first=True)\n",
            "  )\n",
            "  (stack_model): PassThru()\n",
            "  (output_layer): Dense(\n",
            "    (layer): Linear(in_features=600, out_features=2, bias=True)\n",
            "    (activation): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "adadelta(eta=1.000000, wd=0.000000)\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 1, \"phase\": \"Train\", \"acc\": 0.8658801211002976, \"precision\": 0.8756498623820838, \"recall\": 0.8808301190278994, \"f1\": 0.8782323518308796, \"mcc\": 0.7289898781125169, \"avg_loss\": 0.3212745072298394}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 1, \"phase\": \"Valid\", \"acc\": 0.8463302752293578, \"precision\": 0.8725961538461539, \"recall\": 0.8175675675675675, \"f1\": 0.8441860465116279, \"mcc\": 0.694349911562367, \"avg_loss\": 0.3388754332762793}\n",
            "New best 0.846\n",
            "saving sst2-pyt/classify-model-512.pyt\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 2, \"phase\": \"Train\", \"acc\": 0.9173217603721365, \"precision\": 0.9231422104866088, \"recall\": 0.9265718545161977, \"f1\": 0.9248538529672277, \"mcc\": 0.8329731722512385, \"avg_loss\": 0.21677999859696215}\n",
            "Progress: [========================================] 100%\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 2, \"phase\": \"Valid\", \"acc\": 0.841743119266055, \"precision\": 0.8625592417061612, \"recall\": 0.8198198198198198, \"f1\": 0.8406466512702079, \"mcc\": 0.6845435912133538, \"avg_loss\": 0.3444597020471862}\n",
            "Best performance on acc: 0.846 at epoch 0\n",
            "Reloading best checkpoint\n",
            "adadelta(eta=1.000000, wd=0.000000)\n",
            "Progress: [========================================] 100%\n",
            "                 0        1\n",
            "        0      793      119\n",
            "        1      120      789\n",
            "\n",
            "\n",
            "{\"tick_type\": \"EPOCH\", \"tick\": 0, \"phase\": \"Test\", \"acc\": 0.8687534321801208, \"precision\": 0.8689427312775331, \"recall\": 0.8679867986798679, \"f1\": 0.868464501926252, \"mcc\": 0.7375061217913733, \"avg_loss\": 0.306584202837381}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J30fmdLZjLcN",
        "colab_type": "text"
      },
      "source": [
        "You might have also noticed if you looked at the config file, that we didnt actually do what we said, and pass a URL into the addons, instead we did this:\n",
        "\n",
        "```yaml\n",
        "modules: [hub:v1:addons:resconv]\n",
        "```\n",
        "\n",
        "This is just an alias for `addons` that live in [mead-hub](https://github.com/mead-ml/hub).  Think of `hub` as an alias for the github root, and the rest of the `:` are path delimiters up to the file (we dont need to add the suffix as we expect it will be a `.py` file).\n",
        "\n",
        "### Wrap-up\n",
        "\n",
        "In this tutorial, we have demonstrated how to use the baseline models from `mead-baseline` including an LSTM-based model, a convolutional model, and a BERT-fine-tuned model.\n",
        "\n",
        "We also explored how `mead-train` delegates its responsibilities through Inversion of Control to deep-learning framework-specific packages to provide a rich set of models for PyTorch and TensorFlow.  Finally, we explored how to override the existing packages to make our own models, with some assistance from the 8-mile layers provided in `mead-layers`.\n"
      ]
    }
  ]
}